# Training Configuration Templates for TPU

# Default training configuration (TPU optimized)
default:
  # Model
  model_name: "google/gemma-3-270m-it"
  load_in_4bit: false  # 4-bit quantization not well supported on TPU
  
  # Dataset
  dataset_format: "jsonl"
  text_field: "text"
  
  # Training hyperparameters
  learning_rate: 2.0e-5
  num_train_epochs: 3
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # Optimizer (standard for TPU)
  optim: "adamw_torch"  # Use standard torch optimizer for TPU
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Logging & Saving (reduced frequency for TPU)
  logging_steps: 50   # Less frequent to avoid sync overhead
  save_strategy: "steps"
  save_steps: 200     # Less frequent saves
  save_total_limit: 3
  
  # Evaluation (reduced frequency)
  evaluation_strategy: "steps"
  eval_steps: 200     # Less frequent eval
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  
  # Early stopping
  early_stopping_patience: 5
  early_stopping_min_delta: 0.001
  
  # Data split
  train_ratio: 0.80
  val_ratio: 0.10
  test_ratio: 0.10
  
  # Output
  output_dir: "./outputs"

# Google Colab TPU optimized
colab_tpu:
  # Inherit from default
  _extends: default
  
  # TPU Configuration
  tpu_cores: 8              # TPU v2/v3 has 8 cores
  
  # Batch size MUST be multiples of 128 for TPU efficiency
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 128
  gradient_accumulation_steps: 2  # Global batch = 128 * 8 * 2 = 2048
  gradient_checkpointing: true
  
  # Mixed precision - TPU ONLY supports bfloat16!
  bf16: true    # TPU native precision
  fp16: false   # TPU does NOT support fp16
  
  # Performance (TPU specific)
  dataloader_num_workers: 4
  dataloader_pin_memory: false  # Not needed for TPU
  dataloader_drop_last: true    # Important for TPU
  torch_compile: false
