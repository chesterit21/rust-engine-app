{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Fine-Tuning Gemma 3 270M on TPU\n",
    "\n",
    "Production-grade fine-tuning using **Hugging Face Transformers + TRL**\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Setup\n",
    "\n",
    "1. **Runtime** â†’ **Change runtime type** â†’ **TPU**\n",
    "2. Run cells in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hâœ… Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "#%pip install -q torch tensorboard\n",
    "#%pip install -q transformers datasets accelerate evaluate trl protobuf sentencepiece peft\n",
    "\n",
    "# Optional: Flash Attention (for L4/A100)\n",
    "# %pip install -q flash-attn\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Step 2: HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3304907956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Login to Hugging Face (requires accepting Gemma license)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get token from: https://huggingface.co/settings/tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhf_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI."
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face (requires accepting Gemma license)\n",
    "# Get token from: https://huggingface.co/settings/tokens\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)\n",
    "\n",
    "print(\"âœ… Logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration\n",
    "base_model = \"google/gemma-3-270m-it\"  # @param [\"google/gemma-3-270m-it\",\"google/gemma-3-1b-it\",\"google/gemma-3-4b-it\"]\n",
    "checkpoint_dir = \"/content/drive/MyDrive/gemma3-finetuned\"  # @param {type:\"string\"}\n",
    "learning_rate = 5e-5  # @param {type:\"number\"}\n",
    "num_epochs = 3  # @param {type:\"integer\"}\n",
    "max_length = 512  # @param {type:\"integer\"}\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Model: {base_model}\")\n",
    "print(f\"   Output: {checkpoint_dir}\")\n",
    "print(f\"   LR: {learning_rate}\")\n",
    "print(f\"   Epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from google.colab import files\n",
    "\n",
    "# Option A: Upload your own dataset\n",
    "print(\"ğŸ“¤ Upload your dataset (JSONL with 'messages' field):\")\n",
    "uploaded = files.upload()\n",
    "dataset_path = list(uploaded.keys())[0]\n",
    "\n",
    "# Load and convert dataset\n",
    "def create_conversation(sample):\n",
    "    return {\"messages\": sample[\"messages\"]}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset loaded:\")\n",
    "print(f\"   Train: {len(dataset['train'])} samples\")\n",
    "print(f\"   Test: {len(dataset['test'])} samples\")\n",
    "print(f\"\\nğŸ“‹ Sample:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ Step 6: Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model with official Google settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"  # Google recommended\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "print(f\"âœ… Model loaded!\")\n",
    "print(f\"   Device: {model.device}\")\n",
    "print(f\"   DType: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 7: Setup LoRA (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Apply LoRA for memory-efficient training\n",
    "USE_LORA = True  # @param {type:\"boolean\"}\n",
    "\n",
    "if USE_LORA:\n",
    "    from peft import get_peft_model, LoraConfig, TaskType\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"ğŸ“ Full model fine-tuning (no LoRA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 8: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Training config (Official Google pattern)\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=checkpoint_dir,\n",
    "    max_length=max_length,\n",
    "    packing=False,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_checkpointing=False,  # Caching incompatible\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True if torch_dtype == torch.float16 else False,\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model()\n",
    "print(f\"\\nâœ… Training complete! Model saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 9: Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract losses\n",
    "log_history = trainer.state.log_history\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 10: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "test_prompt = \"Hello, how are you?\"  # @param {type:\"string\"}\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"ğŸ“ Prompt: {test_prompt}\")\n",
    "print(f\"\\nğŸ¤– Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Done!\n",
    "\n",
    "Your fine-tuned model is saved at: `{checkpoint_dir}`\n",
    "\n",
    "### Next Steps:\n",
    "1. Download model from Google Drive\n",
    "2. Convert to GGUF for Ollama/LM Studio\n",
    "3. Deploy to production"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
