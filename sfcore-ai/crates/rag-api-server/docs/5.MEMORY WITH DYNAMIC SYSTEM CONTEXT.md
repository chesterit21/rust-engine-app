## ğŸš€ FINAL DESIGN - CONVERSATION MEMORY WITH DYNAMIC SYSTEM CONTEXT

Siap bro! Semua sudah **100% CLEAR dan INLINE**! Sekarang aku finalize design lengkap sebelum eksekusi code.

***

## ğŸ“Š DELIVERABLE 1: COMPLETE ARCHITECTURE FLOW

### **High-Level System Flow**

```mermaid
flowchart TD
    Start([POST /api/chat/stream]) --> Extract[Extract Request Data<br/>user_id, session_id, message, document_id]
    Extract --> GetSession{Session Exists<br/>in Cache?}
    
    GetSession -->|No| CreateSession[Create New Session<br/>session_id, created_at, empty history]
    GetSession -->|Yes| CheckExpiry{Session Expired?<br/>created_at + 6h}
    
    CheckExpiry -->|Yes| CreateSession
    CheckExpiry -->|No| LoadSession[Load Session from Cache]
    
    CreateSession --> CheckWindow
    LoadSession --> CheckWindow
    
    CheckWindow{History Window<br/>>= 5 pairs?}
    CheckWindow -->|Yes| DeleteOldest[DELETE Q1, A1<br/>ALWAYS enforce 5-pair max]
    CheckWindow -->|No| CheckRetrieval
    
    DeleteOldest --> CheckRetrieval
    
    CheckRetrieval{Need Retrieval?<br/>Check document_id & similarity}
    
    CheckRetrieval -->|First Message| DoRetrieval[Execute Retrieval<br/>Embed current message only]
    CheckRetrieval -->|document_id SAMA<br/>AND similarity > 0.75| ReuseContext[Reuse Previous<br/>System Context]
    CheckRetrieval -->|document_id BEDA<br/>OR similarity <= 0.75| ContextAwareRetrieval[Context-Aware Retrieval<br/>Weighted: current 0.7 + history 0.3]
    
    DoRetrieval --> SummarizeChunks[LLM Summarization<br/>Summarize retrieval chunks]
    ContextAwareRetrieval --> SummarizeChunks
    
    SummarizeChunks --> BuildSystem[Build System Context<br/>Base + Summarized Retrieval]
    ReuseContext --> AppendMessage
    
    BuildSystem --> AppendMessage[Append User Message<br/>to History]
    
    AppendMessage --> CountTokens[Count Total Tokens<br/>System + History + Current]
    
    CountTokens --> CheckToken{Total Tokens<br/>> 20K?}
    
    CheckToken -->|No| CheckSafeLimit{Total Tokens<br/>> 23K?}
    CheckToken -->|Yes| CascadeDelete[Cascade Delete<br/>Q1â†’Q2â†’Q3â†’Q4...]
    
    CascadeDelete --> ReCount[Re-count Tokens]
    ReCount --> CheckAfterDelete{Still > 23K?}
    
    CheckAfterDelete -->|Yes| TruncateRetrieval[Force Truncate<br/>Retrieval Results]
    CheckAfterDelete -->|No| PreparePayload
    
    CheckSafeLimit -->|Yes| TruncateRetrieval
    CheckSafeLimit -->|No| PreparePayload
    
    TruncateRetrieval --> PreparePayload[Prepare LLM Payload<br/>messages array]
    
    PreparePayload --> CallLLM[Call LLM API<br/>with retry 3x]
    
    CallLLM --> LLMSuccess{Success?}
    
    LLMSuccess -->|No| RetryCount{Retry < 3?}
    RetryCount -->|Yes| CallLLM
    RetryCount -->|No| ErrorLLM[Return Error:<br/>Server ada gangguan]
    
    LLMSuccess -->|Yes| StreamSSE[Stream SSE Response<br/>to Client]
    
    StreamSSE --> SaveResponse[Save Assistant Response<br/>to History]
    
    SaveResponse --> UpdateCache[Update Memory Cache<br/>last_activity, history, context]
    
    UpdateCache --> End([Response Complete])
    ErrorLLM --> End
```

***

## ğŸ“Š DELIVERABLE 2: DETAILED COMPONENT FLOW

### **Flow 1: Retrieval Decision Logic**

```mermaid
flowchart TD
    Start([New Message Arrives]) --> FirstCheck{First Message<br/>in Session?}
    
    FirstCheck -->|Yes| DirectRetrieval[Execute Retrieval<br/>Embed: current message only]
    FirstCheck -->|No| CheckDocID{document_id<br/>SAMA dengan previous?}
    
    CheckDocID -->|No| TopicJump[Topic Jump Detected<br/>NEW Retrieval needed]
    CheckDocID -->|Yes| CheckSimilarity{Calculate Similarity<br/>current vs last query}
    
    CheckSimilarity --> SimilarityResult{Similarity<br/>> 0.75?}
    
    SimilarityResult -->|No| TopicJump
    SimilarityResult -->|Yes| SkipRetrieval[SKIP Retrieval<br/>Reuse Previous Context]
    
    TopicJump --> WeightedEmbed[Context-Aware Embedding<br/>Concatenate: current + last N msgs<br/>Weight: current 0.7, history 0.3]
    
    WeightedEmbed --> SearchDB[Search Database<br/>pgvector similarity]
    DirectRetrieval --> SearchDB
    
    SearchDB --> GetChunks[Get Top-K Chunks<br/>user-authorized only]
    
    GetChunks --> LLMSummarize[Call LLM to Summarize<br/>Chunks â†’ Concise Context]
    
    LLMSummarize --> BuildNewContext[Build NEW System Context<br/>Replace previous]
    
    SkipRetrieval --> ReuseOldContext[Use Existing System Context<br/>No changes]
    
    BuildNewContext --> Done([Context Ready])
    ReuseOldContext --> Done
```

***

### **Flow 2: Token Management & Deletion**

```mermaid
flowchart TD
    Start([User Message Ready]) --> EnforceWindow{History Window<br/>>= 5 pairs?}
    
    EnforceWindow -->|Yes| ForceDelete[FORCE DELETE Q1, A1<br/>Regardless of token count]
    EnforceWindow -->|No| AppendMsg[Append User Message<br/>to History temporarily]
    
    ForceDelete --> AppendMsg
    
    AppendMsg --> BuildPayload[Build Full Payload:<br/>System + History + Current]
    
    BuildPayload --> EstimateToken[Calculate Total Tokens<br/>2-3 chars = 1 token randomized]
    
    EstimateToken --> Check20K{Total Tokens<br/>> 20K?}
    
    Check20K -->|No| Check23K{Total Tokens<br/>> 23K?}
    Check20K -->|Yes| DeleteQ1[Delete Q1, A1]
    
    DeleteQ1 --> ReCalc1[Re-calculate Tokens]
    ReCalc1 --> StillOver1{Still > 20K?}
    
    StillOver1 -->|Yes| DeleteQ2[Delete Q2, A2]
    StillOver1 -->|No| Check23K
    
    DeleteQ2 --> ReCalc2[Re-calculate Tokens]
    ReCalc2 --> StillOver2{Still > 20K?}
    
    StillOver2 -->|Yes| DeleteQ3[Delete Q3, A3<br/>Continue cascade...]
    StillOver2 -->|No| Check23K
    
    DeleteQ3 --> FinalCheck{Still > 20K?}
    FinalCheck -->|Yes| OnlyCurrentLeft[Only Q6 remains<br/>All history deleted]
    FinalCheck -->|No| Check23K
    
    Check23K -->|Yes| TruncateRetrieval[Truncate Retrieval Results<br/>Keep only essential info]
    Check23K -->|No| PayloadReady
    
    OnlyCurrentLeft --> Check23K
    TruncateRetrieval --> PayloadReady[Payload Ready<br/>Total <= 23K]
    
    PayloadReady --> UpdateMemory[Update Memory Cache<br/>with cleaned history]
    
    UpdateMemory --> SendToLLM([Send to LLM])
```

***

## ğŸ“¦ DELIVERABLE 3: MODULE ORGANIZATION & STRUCTURE

Berdasarkan struktur existing di [GitHub repo](https://github.com/chesterit21/rust-engine-app/tree/master/sfcore-ai/crates/rag-api-server), ini **module baru & modifications**:

### **New Module Structure**

```
src/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ conversation/           # â† NEW MODULE
â”‚   â”‚   â”œâ”€â”€ mod.rs             # Module exports
â”‚   â”‚   â”œâ”€â”€ manager.rs         # ConversationManager (main orchestrator)
â”‚   â”‚   â”œâ”€â”€ cache.rs           # DashMap-based memory cache
â”‚   â”‚   â”œâ”€â”€ context_builder.rs # System context builder
â”‚   â”‚   â”œâ”€â”€ token_counter.rs   # Token counting & estimation
â”‚   â”‚   â””â”€â”€ types.rs           # Data structures
â”‚   â”‚
â”‚   â”œâ”€â”€ embedding.rs           # MODIFY: add weighted embedding
â”‚   â”œâ”€â”€ retrieval.rs           # MODIFY: add context-aware retrieval
â”‚   â”œâ”€â”€ llm.rs                 # MODIFY: add summarization call
â”‚   â””â”€â”€ mod.rs                 # Update exports
â”‚
â”œâ”€â”€ handlers/
â”‚   â””â”€â”€ chat.rs                # MODIFY: integrate ConversationManager
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ chat.rs                # MODIFY: add session_id field
â”‚
â””â”€â”€ utils/
    â””â”€â”€ similarity.rs          # NEW: embedding similarity calculation
```

***

## ğŸ“‹ DELIVERABLE 4: DATA STRUCTURE SPECIFICATION

### **Core Structures**

```
ConversationCache (Global State)
â”œâ”€â”€ Type: Arc<DashMap<i64, ConversationState>>
â”œâ”€â”€ Key: session_id (i64)
â”œâ”€â”€ Value: ConversationState
â””â”€â”€ Thread-safe: Yes (DashMap handles concurrency)

ConversationState
â”œâ”€â”€ session_id: i64                    # Format: yyyyMMddHHmmss + user_id
â”œâ”€â”€ user_id: i64
â”œâ”€â”€ document_id: Option<i64>           # Current document context
â”œâ”€â”€ messages: Vec<ChatMessage>         # Max 10 items (5 pairs)
â”œâ”€â”€ system_context: String             # Current System prompt
â”œâ”€â”€ last_retrieval_summary: String     # Cached summarized context
â”œâ”€â”€ last_query_embedding: Option<Vec<f32>>  # For similarity check
â”œâ”€â”€ created_at: Instant                # For 6-hour expiration
â”œâ”€â”€ last_activity: Instant             # For monitoring
â””â”€â”€ metadata: ConversationMetadata

ChatMessage (OpenAI Format)
â”œâ”€â”€ role: String                       # "user" | "assistant"
â””â”€â”€ content: String                    # Message text

ConversationMetadata
â”œâ”€â”€ total_messages: usize              # Counter
â”œâ”€â”€ total_retrievals: usize            # Counter
â”œâ”€â”€ total_tokens_last: usize           # Last known token count
â””â”€â”€ retrieval_skipped_count: usize     # How many times reused context

SystemContextComponents
â”œâ”€â”€ base_instruction: String           # Fixed template
â”œâ”€â”€ retrieval_context: String          # Summarized chunks (dynamic)
â””â”€â”€ metadata_section: String           # Document info (optional)
```

***

## ğŸ“¦ DELIVERABLE 5: LIBRARY DEPENDENCIES

### **New Dependencies untuk Cargo.toml**

```
[dependencies]
# Existing dependencies (keep as is)
# axum, tokio, sqlx, serde, etc...

# === NEW: Conversation Memory ===
dashmap = "6.1"              # Thread-safe concurrent HashMap
                             # Lock-free reads, perfect for cache

# === NEW: Token Counting ===
# Option 1: Approximate (lightweight)
unicode-segmentation = "1.12" # For accurate char counting

# Option 2: Accurate (heavier)
tiktoken-rs = "0.5"          # OpenAI tokenizer (if needed later)

# === NEW: Similarity Calculation ===
# Already have vector operations from embedding service
# Use existing nalgebra or ndarray if available

# === NEW: Memory Management ===
sysinfo = "0.31"             # For RAM monitoring (90% limit check)

# === EXISTING: Enhance ===
# reqwest (already have) - for LLM summarization calls
# chrono (already have) - for session_id generation
```

### **Library Reasoning**

**DashMap vs Alternatives:**

- âœ… **DashMap (CHOSEN)**: Lock-free reads, zero-cost, perfect fit
- âŒ Moka: Overkill (built-in TTL not needed, we do lazy deletion)
- âŒ RwLock + HashMap: Need manual locking, slower

**Token Counting:**

- âœ… **unicode-segmentation (CHOSEN)**: Lightweight, flexible for 2-3 char rule
- âŒ tiktoken-rs: Accurate but heavier, overkill untuk approximate counting

**RAM Monitoring:**

- âœ… **sysinfo (CHOSEN)**: Cross-platform, simple API untuk check RAM usage

***

## ğŸ¯ DELIVERABLE 6: INTEGRATION POINTS

### **Existing Code â†’ Modifications Needed**

**1. Handler: `src/handlers/chat.rs`**

```
BEFORE:
- Direct call to retrieval service
- Direct call to LLM
- No conversation state

AFTER:
- Call ConversationManager.handle_message()
- ConversationManager orchestrates everything
- Return SSE stream as before (no breaking changes)
```

**2. Service: `src/services/embedding.rs`**

```
ADDITIONS:
- weighted_embed() function
- Input: (current_text: String, history_texts: Vec<String>, weights: (f32, f32))
- Output: Vec<f32> (weighted embedding)
```

**3. Service: `src/services/llm.rs`**

```
ADDITIONS:
- summarize_chunks() function
- Input: Vec<RetrievalChunk>
- Output: String (summarized context)
- Retry logic: 3x with exponential backoff
```

**4. Models: `src/models/chat.rs`**

```
MODIFY ChatRequest:
- Add field: session_id: i64
- Keep existing: user_id, message, document_id
```

***

## ğŸ§ª DELIVERABLE 7: CRITICAL EDGE CASES MATRIX

| Scenario | Input | Expected Behavior | Validation |
|----------|-------|-------------------|------------|
| **Normal Flow** | 5 sequential messages, same doc_id | - First: retrieval<br/>- Rest: skip (reuse context)<br/>- Window: 5 pairs maintained | All messages <= 20K, no deletion |
| **Window Enforcement** | Message 6 arrives, token = 8K | - FORCE delete Q1,A1<br/>- Regardless of token count | History has max 5 pairs |
| **Token Overflow (Moderate)** | Message 5, total = 22K | - Delete Q1,A1<br/>- Re-count = 18K<br/>- Send to LLM | Payload <= 23K |
| **Token Overflow (Extreme)** | Message 6, total = 27K | - Cascade delete Q1â†’Q2â†’Q3<br/>- Truncate retrieval if needed | Final payload <= 23K |
| **Topic Jump** | Message 4, different doc_id | - New retrieval (context-aware)<br/>- Replace System context<br/>- KEEP history | History not cleared |
| **Similarity Border** | Message 3, similarity = 0.75 | - Check: doc_id SAME AND sim >= 0.75<br/>- Skip retrieval | Reuse previous context |
| **First Message** | New session, no history | - Create session<br/>- Retrieval: current only<br/>- No weighted embedding | Cache created properly |
| **Session Expiry** | Request after 6+ hours | - Treat as new session<br/>- Clear old cache entry<br/>- Fresh retrieval | No stale data used |
| **DB Retrieval Fail** | Database timeout on retrieval | - Retry 3x<br/>- Return error message<br/>- Do NOT save to history | User notified properly |
| **LLM Timeout** | LLM API timeout | - Retry 3x<br/>- Return error message<br/>- Do NOT save to history | Graceful failure |
| **Concurrent Requests** | 2 requests, same session_id | - DashMap handles atomically<br/>- Sequential processing<br/>- No race condition | Data consistency maintained |
| **RAM Limit** | Active sessions â†’ 90% RAM | - Reject new session creation<br/>- Return error<br/>- Existing sessions continue | System stability |

***

## ğŸ¬ DELIVERABLE 8: IMPLEMENTATION PLAN

### **Phase 1: Core Infrastructure**

1. âœ… Create `services/conversation/` module structure
2. âœ… Implement `ConversationCache` with DashMap
3. âœ… Define all data structures in `types.rs`
4. âœ… Implement token counter (2-3 char randomized logic)

### **Phase 2: Conversation Manager**

1. âœ… Implement `ConversationManager.handle_message()` (main entry point)
2. âœ… Session lifecycle (create, get, update, expire check)
3. âœ… Sliding window logic (enforce 5-pair max)
4. âœ… Token management & cascade deletion

### **Phase 3: Context Building**

1. âœ… Implement `ContextBuilder` module
2. âœ… Retrieval decision logic (document_id + similarity check)
3. âœ… Weighted embedding for context-aware retrieval
4. âœ… LLM summarization integration

### **Phase 4: Integration**

1. âœ… Modify `handlers/chat.rs` to use ConversationManager
2. âœ… Update `models/chat.rs` with session_id
3. âœ… Enhance `services/embedding.rs` with weighted function
4. âœ… Enhance `services/llm.rs` with summarization

### **Phase 5: Error Handling & Monitoring**

1. âœ… Retry logic (retrieval & LLM)
2. âœ… RAM monitoring (90% limit)
3. âœ… Graceful error messages
4. âœ… Logging & metrics

***

## âœ… FINAL CONFIRMATION CHECKLIST

Sebelum aku mulai coding, konfirmasi terakhir:

- âœ… Similarity logic: **AND** (document_id SAMA AND similarity > 0.75)
- âœ… Weighted embedding: **Option A** (Concatenate then weight)
- âœ… Summarization: **Option B** (LLM call untuk summarize chunks)
- âœ… Sliding window: **ALWAYS** enforce 5-pair max
- âœ… Token limit: **20K soft, 23K hard** (model max 32K)
- âœ… Session expiry: **6 hours from created_at** (absolute)
- âœ… Retry: **3x** untuk DB dan LLM errors
- âœ… Structure: **Follow existing GitHub repo** pattern

***

## ğŸš€ READY TO CODE

**SEMUA SUDAH INLINE BRO!** ğŸ¯

Sekarang akan implement **FULL CODE LENGKAP** dengan breakdown per module:

1. âœ… `services/conversation/types.rs` - Data structures
2. âœ… `services/conversation/cache.rs` - DashMap wrapper
3. âœ… `services/conversation/token_counter.rs` - Token counting logic
4. âœ… `services/conversation/context_builder.rs` - System context builder
5. âœ… `services/conversation/manager.rs` - Main orchestrator
6. âœ… `utils/similarity.rs` - Embedding similarity
7. âœ… Modifications to existing files (handlers, services, models)
8. âœ… Updated `Cargo.toml` with dependencies

**GASKEUN! SEMANGAT! ğŸš€ğŸ¦€**
