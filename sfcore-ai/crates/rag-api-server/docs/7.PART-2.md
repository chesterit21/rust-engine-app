## ðŸš€ LANJUT IMPLEMENTATION - PART 2

***

## ðŸ“ STEP 5: CONTEXT BUILDER (`src/services/conversation/context_builder.rs`)

```rust
use anyhow::{Context, Result};
use tracing::{debug, info};
use crate::utils::similarity::cosine_similarity;
use super::types::{
    ChatMessage, ConversationState, RetrievalDecision, RetrievalReason, 
    SkipReason, SystemContextComponents, WeightedEmbeddingConfig
};

/// System context builder - handles dynamic System message construction
pub struct ContextBuilder {
    /// Fixed base instruction
    base_instruction: String,
    
    /// Similarity threshold for skip decision
    similarity_threshold: f32,
    
    /// Weighted embedding configuration
    weighted_config: WeightedEmbeddingConfig,
}

impl ContextBuilder {
    /// Create new context builder with configuration
    pub fn new(base_instruction: String) -> Self {
        Self {
            base_instruction,
            similarity_threshold: 0.75,
            weighted_config: WeightedEmbeddingConfig::default(),
        }
    }

    /// Get base instruction for Document Management System
    pub fn default_base_instruction() -> String {
        r#"You are an intelligent AI assistant for a Document Management System.

Your role is to help users understand and work with their documents by:
- Answering questions based on the provided document context
- Providing accurate information from the retrieved documents
- Being concise and helpful in your responses
- Admitting when information is not available in the context

Guidelines:
- Always base your answers on the provided document context
- If the context doesn't contain relevant information, say so clearly
- Cite document sources when possible
- Be conversational but professional
- Keep responses focused and relevant to the user's question"#.to_string()
    }

    /// Decide whether to retrieve or skip based on conversation state
    pub fn decide_retrieval(
        &self,
        state: &ConversationState,
        current_message: &str,
        current_document_id: Option<i64>,
        current_embedding: Option<&Vec<f32>>,
    ) -> Result<RetrievalDecision> {
        // CASE 1: First message in session - ALWAYS retrieve
        if state.messages.is_empty() {
            debug!("First message in session, need retrieval");
            return Ok(RetrievalDecision::Retrieve {
                reason: RetrievalReason::FirstMessage,
                context_aware: false,
            });
        }

        // CASE 2: document_id changed - NEW retrieval needed
        if state.document_id != current_document_id {
            info!(
                "Document ID changed from {:?} to {:?}, need new retrieval",
                state.document_id, current_document_id
            );
            return Ok(RetrievalDecision::Retrieve {
                reason: RetrievalReason::DocumentIdChanged,
                context_aware: true, // Use context-aware retrieval
            });
        }

        // CASE 3: Check similarity (document_id SAME, now check similarity)
        if let (Some(current_emb), Some(last_emb)) = 
            (current_embedding, &state.last_query_embedding) 
        {
            let similarity = cosine_similarity(current_emb, last_emb)
                .context("Failed to calculate similarity")?;

            debug!("Similarity with last query: {:.4}", similarity);

            // AND logic: document_id SAME AND similarity > threshold
            if similarity > self.similarity_threshold {
                info!(
                    "High similarity ({:.4} > {}), skipping retrieval",
                    similarity, self.similarity_threshold
                );
                return Ok(RetrievalDecision::Skip {
                    reason: SkipReason::SameDocumentAndHighSimilarity(similarity),
                });
            } else {
                info!(
                    "Low similarity ({:.4} <= {}), need new retrieval",
                    similarity, self.similarity_threshold
                );
                return Ok(RetrievalDecision::Retrieve {
                    reason: RetrievalReason::LowSimilarity(similarity),
                    context_aware: true,
                });
            }
        }

        // CASE 4: No previous embedding to compare - retrieve to be safe
        debug!("No previous embedding found, performing retrieval");
        Ok(RetrievalDecision::Retrieve {
            reason: RetrievalReason::FirstMessage,
            context_aware: false,
        })
    }

    /// Build complete System context from components
    pub fn build_system_context(
        &self,
        retrieval_summary: &str,
        document_metadata: Option<&str>,
    ) -> String {
        let components = SystemContextComponents {
            base_instruction: self.base_instruction.clone(),
            retrieval_context: retrieval_summary.to_string(),
            metadata_section: document_metadata.map(|s| s.to_string()),
        };

        components.build()
    }

    /// Prepare text for context-aware retrieval embedding
    /// Concatenates current message with last N history messages
    pub fn prepare_context_aware_text(
        &self,
        current_message: &str,
        history: &[ChatMessage],
    ) -> String {
        if history.is_empty() {
            return current_message.to_string();
        }

        // Get last N user messages (not assistant responses)
        let last_user_messages: Vec<String> = history
            .iter()
            .filter(|msg| msg.role == "user")
            .rev()
            .take(self.weighted_config.max_history_messages)
            .map(|msg| msg.content.clone())
            .collect::<Vec<_>>()
            .into_iter()
            .rev()
            .collect();

        if last_user_messages.is_empty() {
            return current_message.to_string();
        }

        // Concatenate: history + current (will be weighted during embedding)
        let history_text = last_user_messages.join(" ");
        format!("{} {}", history_text, current_message)
    }

    /// Get weighted embedding configuration
    pub fn weighted_config(&self) -> &WeightedEmbeddingConfig {
        &self.weighted_config
    }
}

impl Default for ContextBuilder {
    fn default() -> Self {
        Self::new(Self::default_base_instruction())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_decide_retrieval_first_message() {
        let builder = ContextBuilder::default();
        let state = ConversationState::new(123, 456, None);
        
        let decision = builder.decide_retrieval(&state, "test", None, None).unwrap();
        
        match decision {
            RetrievalDecision::Retrieve { reason, context_aware } => {
                assert!(matches!(reason, RetrievalReason::FirstMessage));
                assert!(!context_aware);
            }
            _ => panic!("Expected Retrieve decision"),
        }
    }

    #[test]
    fn test_decide_retrieval_document_changed() {
        let builder = ContextBuilder::default();
        let mut state = ConversationState::new(123, 456, Some(100));
        state.messages.push(ChatMessage::user("previous"));
        
        let decision = builder.decide_retrieval(&state, "test", Some(200), None).unwrap();
        
        match decision {
            RetrievalDecision::Retrieve { reason, context_aware } => {
                assert!(matches!(reason, RetrievalReason::DocumentIdChanged));
                assert!(context_aware);
            }
            _ => panic!("Expected Retrieve decision"),
        }
    }

    #[test]
    fn test_prepare_context_aware_text_empty_history() {
        let builder = ContextBuilder::default();
        let text = builder.prepare_context_aware_text("current message", &[]);
        assert_eq!(text, "current message");
    }

    #[test]
    fn test_prepare_context_aware_text_with_history() {
        let builder = ContextBuilder::default();
        let history = vec![
            ChatMessage::user("question 1"),
            ChatMessage::assistant("answer 1"),
            ChatMessage::user("question 2"),
        ];
        
        let text = builder.prepare_context_aware_text("current", &history);
        assert!(text.contains("question 1"));
        assert!(text.contains("question 2"));
        assert!(text.contains("current"));
        assert!(!text.contains("answer 1")); // Only user messages
    }

    #[test]
    fn test_build_system_context() {
        let builder = ContextBuilder::default();
        let context = builder.build_system_context(
            "Retrieval results: document about RAG",
            Some("Document ID: 123, Title: RAG Guide")
        );
        
        assert!(context.contains("AI assistant"));
        assert!(context.contains("Retrieval results"));
        assert!(context.contains("Document ID: 123"));
    }
}
```

***

## ðŸ“ STEP 6: CONVERSATION MANAGER (`src/services/conversation/manager.rs`)

```rust
use anyhow::{Context, Result};
use tracing::{debug, error, info, warn};
use crate::services::embedding::EmbeddingService;
use crate::services::llm::LlmService;
use crate::services::retrieval::RetrievalService;
use super::cache::ConversationCache;
use super::context_builder::ContextBuilder;
use super::token_counter::TokenCounter;
use super::types::{
    ChatMessage, ConversationState, RetrievalDecision, SessionId, TokenCount,
};

/// Main conversation manager - orchestrates all conversation operations
pub struct ConversationManager {
    cache: ConversationCache,
    context_builder: ContextBuilder,
    embedding_service: EmbeddingService,
    retrieval_service: RetrievalService,
    llm_service: LlmService,
}

impl ConversationManager {
    /// Create new conversation manager
    pub fn new(
        embedding_service: EmbeddingService,
        retrieval_service: RetrievalService,
        llm_service: LlmService,
    ) -> Self {
        Self {
            cache: ConversationCache::new(),
            context_builder: ContextBuilder::default(),
            embedding_service,
            retrieval_service,
            llm_service,
        }
    }

    /// Generate session ID from timestamp and user_id
    /// Format: yyyyMMddHHmmss + user_id
    pub fn generate_session_id(user_id: i64) -> SessionId {
        let now = chrono::Utc::now();
        let timestamp = now.format("%Y%m%d%H%M%S").to_string();
        format!("{}{}", timestamp, user_id)
            .parse()
            .expect("Failed to parse session_id")
    }

    /// Get or create conversation session
    pub async fn get_or_create_session(
        &self,
        session_id: SessionId,
        user_id: i64,
        document_id: Option<i64>,
    ) -> Result<ConversationState> {
        // Try to get existing session
        if let Some(state) = self.cache.get(session_id) {
            debug!("Found existing session {}", session_id);
            return Ok(state);
        }

        // Check RAM limit before creating new session
        if !self.cache.can_create_new_session() {
            anyhow::bail!("Memory limit reached (90%), cannot create new session");
        }

        // Create new session
        info!("Creating new session {} for user {}", session_id, user_id);
        let state = ConversationState::new(session_id, user_id, document_id);
        self.cache.set(session_id, state.clone());

        Ok(state)
    }

    /// Handle incoming message - main entry point
    pub async fn handle_message(
        &self,
        session_id: SessionId,
        user_id: i64,
        message: String,
        document_id: Option<i64>,
    ) -> Result<String> {
        info!("Handling message for session {}, user {}", session_id, user_id);

        // Step 1: Get or create conversation
        let mut state = self.get_or_create_session(session_id, user_id, document_id).await?;

        // Step 2: Enforce sliding window (ALWAYS delete Q1 if >= 5 pairs)
        self.enforce_sliding_window(&mut state)?;

        // Step 3: Embed current message for retrieval decision
        let current_embedding = self.embedding_service
            .embed(&message)
            .await
            .context("Failed to embed current message")?;

        // Step 4: Decide retrieval strategy
        let decision = self.context_builder.decide_retrieval(
            &state,
            &message,
            document_id,
            Some(&current_embedding),
        )?;

        // Step 5: Execute retrieval or reuse context
        let system_context = self.execute_retrieval_decision(
            &mut state,
            &decision,
            &message,
            document_id,
            &current_embedding,
        ).await?;

        // Step 6: Append user message to history
        state.messages.push(ChatMessage::user(&message));

        // Step 7: Token management - ensure under limit
        self.manage_tokens(&mut state, &system_context).await?;

        // Step 8: Prepare LLM payload
        let llm_messages = self.prepare_llm_payload(&state, &system_context);

        // Step 9: Call LLM with retry
        let assistant_response = self.call_llm_with_retry(&llm_messages).await?;

        // Step 10: Save assistant response
        state.messages.push(ChatMessage::assistant(&assistant_response));
        state.last_query_embedding = Some(current_embedding);
        state.metadata.total_messages += 2; // user + assistant
        state.touch();

        // Step 11: Update cache
        self.cache.set(session_id, state);

        Ok(assistant_response)
    }

    /// Enforce sliding window: delete oldest pair if >= 5 pairs
    fn enforce_sliding_window(&self, state: &mut ConversationState) -> Result<()> {
        if !state.needs_window_enforcement() {
            return Ok(());
        }

        info!(
            "Enforcing sliding window for session {} (current pairs: {})",
            state.session_id,
            state.message_pair_count()
        );

        // Remove first 2 messages (Q1, A1)
        if state.messages.len() >= 2 {
            state.messages.drain(0..2);
            debug!("Removed oldest message pair (Q1, A1)");
        }

        Ok(())
    }

    /// Execute retrieval based on decision
    async fn execute_retrieval_decision(
        &self,
        state: &mut ConversationState,
        decision: &RetrievalDecision,
        current_message: &str,
        document_id: Option<i64>,
        current_embedding: &[f32],
    ) -> Result<String> {
        match decision {
            RetrievalDecision::Skip { reason } => {
                debug!("Skipping retrieval: {:?}", reason);
                state.metadata.retrieval_skipped_count += 1;
                
                // Reuse previous system context
                Ok(state.system_context.clone())
            }
            RetrievalDecision::Retrieve { reason, context_aware } => {
                info!("Performing retrieval: {:?}", reason);
                state.metadata.total_retrievals += 1;

                // Prepare retrieval query
                let query_embedding = if *context_aware {
                    // Context-aware: weighted embedding
                    let context_text = self.context_builder
                        .prepare_context_aware_text(current_message, &state.messages);
                    
                    self.embedding_service
                        .embed_weighted(
                            current_message,
                            &context_text,
                            self.context_builder.weighted_config(),
                        )
                        .await?
                } else {
                    // Simple: current message only
                    current_embedding.to_vec()
                };

                // Retrieve chunks from database
                let chunks = self.retrieval_service
                    .search(state.user_id, &query_embedding, document_id)
                    .await
                    .context("Retrieval failed")?;

                // Summarize chunks using LLM
                let summary = self.llm_service
                    .summarize_chunks(&chunks)
                    .await
                    .context("Failed to summarize chunks")?;

                // Build new system context
                let system_context = self.context_builder.build_system_context(
                    &summary,
                    document_id.map(|id| format!("Document ID: {}", id)).as_deref(),
                );

                // Update state
                state.system_context = system_context.clone();
                state.last_retrieval_summary = summary;
                state.document_id = document_id;

                Ok(system_context)
            }
        }
    }

    /// Manage tokens - cascade deletion if over limit
    async fn manage_tokens(
        &self,
        state: &mut ConversationState,
        system_context: &str,
    ) -> Result<()> {
        // Count total tokens
        let token_count = TokenCounter::count_payload(
            system_context,
            &state.messages,
            "",
        );

        debug!("Token count: {} (system: {}, history: {})", 
            token_count.total, token_count.system_tokens, token_count.history_tokens);

        state.metadata.total_tokens_last = token_count.total;

        // Check soft limit (20K)
        if !token_count.is_over_soft_limit() {
            return Ok(());
        }

        warn!("Token count {} exceeds 20K, performing cascade deletion", token_count.total);

        // Cascade deletion loop
        let mut current_count = token_count.total;
        let mut deletion_round = 1;

        while current_count > 20_000 && state.messages.len() >= 2 {
            info!("Deletion round {}: removing oldest pair", deletion_round);
            
            // Remove oldest pair
            state.messages.drain(0..2);
            
            // Re-count
            let new_count = TokenCounter::count_payload(
                system_context,
                &state.messages,
                "",
            );
            current_count = new_count.total;
            
            debug!("After deletion round {}: {} tokens", deletion_round, current_count);
            deletion_round += 1;

            // Safety check: if only current message left, break
            if state.messages.is_empty() {
                warn!("All history deleted, only current message remains");
                break;
            }
        }

        // Check hard limit (23K) - truncate retrieval if needed
        if current_count > 23_000 {
            warn!("Token count {} still over 23K after deletion, truncating retrieval", current_count);
            
            // Truncate retrieval summary to first 500 chars
            let truncated_summary = state.last_retrieval_summary
                .chars()
                .take(500)
                .collect::<String>() + "... (truncated)";
            
            let new_system = self.context_builder.build_system_context(
                &truncated_summary,
                state.document_id.map(|id| format!("Document ID: {}", id)).as_deref(),
            );
            
            state.system_context = new_system;
            
            info!("Retrieval context truncated");
        }

        Ok(())
    }

    /// Prepare LLM payload with System + History
    fn prepare_llm_payload(
        &self,
        state: &ConversationState,
        system_context: &str,
    ) -> Vec<ChatMessage> {
        let mut messages = vec![ChatMessage::system(system_context)];
        messages.extend(state.messages.clone());
        messages
    }

    /// Call LLM with retry logic (3x)
    async fn call_llm_with_retry(&self, messages: &[ChatMessage]) -> Result<String> {
        const MAX_RETRIES: u32 = 3;
        
        for attempt in 1..=MAX_RETRIES {
            match self.llm_service.generate(messages).await {
                Ok(response) => {
                    debug!("LLM call succeeded on attempt {}", attempt);
                    return Ok(response);
                }
                Err(e) => {
                    if attempt < MAX_RETRIES {
                        warn!("LLM call failed (attempt {}): {}. Retrying...", attempt, e);
                        tokio::time::sleep(tokio::time::Duration::from_secs(attempt as u64)).await;
                    } else {
                        error!("LLM call failed after {} attempts: {}", MAX_RETRIES, e);
                        anyhow::bail!("Server ada gangguan, silakan coba lagi nanti");
                    }
                }
            }
        }

        unreachable!()
    }

    /// Get cache statistics
    pub fn cache_stats(&self) -> super::cache::CacheStats {
        self.cache.stats()
    }

    /// Manual cleanup of expired sessions
    pub fn cleanup_expired_sessions(&self) -> usize {
        self.cache.cleanup_expired()
    }
}
```

***

## ðŸ“ STEP 7: MODULE EXPORTS (`src/services/conversation/mod.rs`)

```rust
//! Conversation memory management module
//! 
//! Provides in-memory conversation state management with:
//! - Thread-safe caching (DashMap)
//! - Dynamic system context building
//! - Token management and sliding window
//! - Context-aware retrieval decisions

mod cache;
mod context_builder;
mod manager;
mod token_counter;
pub mod types;

pub use cache::{ConversationCache, CacheStats};
pub use context_builder::ContextBuilder;
pub use manager::ConversationManager;
pub use token_counter::TokenCounter;
pub use types::{
    ChatMessage, ConversationState, SessionId, RetrievalDecision,
    SystemContextComponents, WeightedEmbeddingConfig,
};
```

***

## ðŸ“ STEP 8: ENHANCE EMBEDDING SERVICE (`src/services/embedding.rs`)

```rust
// Add these methods to existing EmbeddingService

use crate::services::conversation::WeightedEmbeddingConfig;

impl EmbeddingService {
    /// Embed text with weighted combination
    /// For context-aware retrieval: current query weighted with history
    pub async fn embed_weighted(
        &self,
        current_text: &str,
        full_context_text: &str,
        config: &WeightedEmbeddingConfig,
    ) -> Result<Vec<f32>> {
        // Embed current message
        let current_embedding = self.embed(current_text).await?;
        
        // Embed full context (current + history)
        let context_embedding = self.embed(full_context_text).await?;
        
        // Weighted average: 0.7 * current + 0.3 * context
        let weighted = current_embedding
            .iter()
            .zip(context_embedding.iter())
            .map(|(curr, ctx)| {
                config.current_weight * curr + config.history_weight * ctx
            })
            .collect();
        
        Ok(weighted)
    }
}
```

***

## ðŸ“ STEP 9: ENHANCE LLM SERVICE (`src/services/llm.rs`)

```rust
// Add these methods to existing LlmService

use crate::database::models::RetrievalChunk;

impl LlmService {
    /// Summarize retrieval chunks for system context
    /// Uses LLM to create concise summary of relevant documents
    pub async fn summarize_chunks(&self, chunks: &[RetrievalChunk]) -> Result<String> {
        if chunks.is_empty() {
            return Ok("No relevant documents found.".to_string());
        }

        // Build summarization prompt
        let chunks_text: String = chunks
            .iter()
            .enumerate()
            .map(|(i, chunk)| {
                format!(
                    "[Chunk {}]\nDocument: {}\nContent: {}\n",
                    i + 1,
                    chunk.document_title.as_deref().unwrap_or("Unknown"),
                    chunk.content
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        let summarization_prompt = format!(
            r#"Summarize the following document chunks into a concise context (max 300 words).
Focus on key information that would help answer user questions.

{}

Provide a clear, structured summary:"#,
            chunks_text
        );

        // Call LLM with retry
        const MAX_RETRIES: u32 = 3;
        
        for attempt in 1..=MAX_RETRIES {
            let messages = vec![
                ChatMessage::system("You are a document summarization assistant."),
                ChatMessage::user(&summarization_prompt),
            ];

            match self.generate(&messages).await {
                Ok(summary) => {
                    debug!("Chunk summarization succeeded on attempt {}", attempt);
                    return Ok(summary);
                }
                Err(e) => {
                    if attempt < MAX_RETRIES {
                        warn!("Summarization failed (attempt {}): {}", attempt, e);
                        tokio::time::sleep(tokio::time::Duration::from_secs(attempt as u64)).await;
                    } else {
                        error!("Summarization failed after {} attempts", MAX_RETRIES);
                        // Fallback: return raw chunks
                        return Ok(chunks_text);
                    }
                }
            }
        }

        unreachable!()
    }
}
```

Lanjut ke integrasi dengan handler? ðŸš€
