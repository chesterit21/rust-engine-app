## ðŸ” CEK CIRCULAR DEPENDENCY - ANALISIS & PERBAIKAN

## âŒ MASALAH CIRCULAR DEPENDENCY YANG TERDETEKSI

### **Problem 1: services/conversation â†” services/embedding/llm/retrieval**

```
src/services/conversation/manager.rs
  â†“ uses
src/services/embedding.rs
src/services/llm.rs
src/services/retrieval.rs
  â†“ (potentially) uses
src/services/conversation/types.rs  âŒ CIRCULAR!
```

### **Problem 2: ChatMessage Type Duplication**

```
src/services/conversation/types.rs -> ChatMessage
src/models/chat.rs -> mungkin juga define ChatMessage
```

***

## âœ… SOLUSI: RESTRUCTURE MODULE HIERARCHY

### **Strategi Perbaikan:**

1. **Move ChatMessage ke models/** - shared type, bukan punya conversation
2. **Remove direct service dependencies** - use trait injection pattern
3. **Proper module hierarchy** - types di bawah, implementasi di atas

***

## ðŸ”§ FIXED CODE - PART 1: RESTRUCTURE

### **STEP 1: UPDATE `src/models/chat.rs`** (Shared Types)

```rust
use serde::{Deserialize, Serialize};

/// OpenAI-compatible message format (SHARED across all modules)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub role: String,      // "user" | "assistant" | "system"
    pub content: String,
}

impl ChatMessage {
    pub fn user(content: impl Into<String>) -> Self {
        Self {
            role: "user".to_string(),
            content: content.into(),
        }
    }

    pub fn assistant(content: impl Into<String>) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.into(),
        }
    }

    pub fn system(content: impl Into<String>) -> Self {
        Self {
            role: "system".to_string(),
            content: content.into(),
        }
    }

    /// Estimate token count for this message
    pub fn estimate_tokens(&self) -> usize {
        use unicode_segmentation::UnicodeSegmentation;
        
        let role_chars = self.role.graphemes(true).count();
        let content_chars = self.content.graphemes(true).count();
        let total_chars = role_chars + content_chars;
        
        // Random 2-3 chars per token + overhead
        let chars_per_token = if rand::random::<bool>() { 2 } else { 3 };
        ((total_chars + chars_per_token - 1) / chars_per_token).max(1) + 3
    }
}

/// Session ID type
pub type SessionId = i64;

/// Chat request payload
#[derive(Debug, Deserialize)]
pub struct ChatRequest {
    pub user_id: i64,
    pub session_id: SessionId,
    pub message: String,
    pub document_id: Option<i64>,
}

/// Source information for citations
#[derive(Debug, Serialize, Clone)]
pub struct SourceInfo {
    pub document_id: i64,
    pub document_title: String,
    pub chunk_id: i64,
    pub similarity: f32,
}
```

***

### **STEP 2: FIXED `src/services/conversation/types.rs`**

```rust
use std::time::Instant;
// IMPORT ChatMessage from models (shared)
use crate::models::chat::{ChatMessage, SessionId};

/// Complete conversation state stored in memory cache
#[derive(Debug, Clone)]
pub struct ConversationState {
    pub session_id: SessionId,
    pub user_id: i64,
    pub document_id: Option<i64>,
    pub messages: Vec<ChatMessage>,  // Using shared ChatMessage from models
    pub system_context: String,
    pub last_retrieval_summary: String,
    pub last_query_embedding: Option<Vec<f32>>,
    pub created_at: Instant,
    pub last_activity: Instant,
    pub metadata: ConversationMetadata,
}

impl ConversationState {
    pub fn new(session_id: SessionId, user_id: i64, document_id: Option<i64>) -> Self {
        let now = Instant::now();
        Self {
            session_id,
            user_id,
            document_id,
            messages: Vec::with_capacity(10),
            system_context: String::new(),
            last_retrieval_summary: String::new(),
            last_query_embedding: None,
            created_at: now,
            last_activity: now,
            metadata: ConversationMetadata::default(),
        }
    }

    pub fn is_expired(&self) -> bool {
        const SIX_HOURS_SECS: u64 = 6 * 60 * 60;
        self.created_at.elapsed().as_secs() > SIX_HOURS_SECS
    }

    pub fn touch(&mut self) {
        self.last_activity = Instant::now();
    }

    pub fn message_pair_count(&self) -> usize {
        self.messages.len() / 2
    }

    pub fn needs_window_enforcement(&self) -> bool {
        self.message_pair_count() >= 5
    }
}

#[derive(Debug, Clone, Default)]
pub struct ConversationMetadata {
    pub total_messages: usize,
    pub total_retrievals: usize,
    pub retrieval_skipped_count: usize,
    pub total_tokens_last: usize,
}

/// System context components
#[derive(Debug, Clone)]
pub struct SystemContextComponents {
    pub base_instruction: String,
    pub retrieval_context: String,
    pub metadata_section: Option<String>,
}

impl SystemContextComponents {
    pub fn build(&self) -> String {
        let mut parts = vec![
            self.base_instruction.clone(),
            String::new(),
            self.retrieval_context.clone(),
        ];

        if let Some(metadata) = &self.metadata_section {
            parts.push(String::new());
            parts.push(metadata.clone());
        }

        parts.join("\n")
    }
}

/// Token counting result
#[derive(Debug, Clone)]
pub struct TokenCount {
    pub total: usize,
    pub system_tokens: usize,
    pub history_tokens: usize,
    pub current_message_tokens: usize,
}

impl TokenCount {
    pub fn is_over_soft_limit(&self) -> bool {
        self.total > 20_000
    }

    pub fn is_over_hard_limit(&self) -> bool {
        self.total > 23_000
    }
}

/// Retrieval decision result
#[derive(Debug, Clone)]
pub enum RetrievalDecision {
    Retrieve {
        reason: RetrievalReason,
        context_aware: bool,
    },
    Skip {
        reason: SkipReason,
    },
}

#[derive(Debug, Clone)]
pub enum RetrievalReason {
    FirstMessage,
    DocumentIdChanged,
    LowSimilarity(f32),
}

#[derive(Debug, Clone)]
pub enum SkipReason {
    SameDocumentAndHighSimilarity(f32),
}

/// Weighted embedding configuration
#[derive(Debug, Clone)]
pub struct WeightedEmbeddingConfig {
    pub current_weight: f32,
    pub history_weight: f32,
    pub max_history_messages: usize,
}

impl Default for WeightedEmbeddingConfig {
    fn default() -> Self {
        Self {
            current_weight: 0.7,
            history_weight: 0.3,
            max_history_messages: 5,
        }
    }
}
```

***

### **STEP 3: FIXED `src/services/conversation/token_counter.rs`**

```rust
use unicode_segmentation::UnicodeSegmentation;
use rand::Rng;
use crate::models::chat::ChatMessage;  // Import from models
use super::types::TokenCount;

pub struct TokenCounter;

impl TokenCounter {
    pub fn count_text(text: &str) -> usize {
        if text.is_empty() {
            return 0;
        }

        let char_count = text.graphemes(true).count();
        let mut rng = rand::thread_rng();
        let chars_per_token = if rng.gen_bool(0.5) { 2 } else { 3 };
        
        ((char_count + chars_per_token - 1) / chars_per_token).max(1)
    }

    pub fn count_messages(messages: &[ChatMessage]) -> usize {
        messages.iter()
            .map(|msg| msg.estimate_tokens())  // Use method from ChatMessage
            .sum()
    }

    pub fn count_payload(
        system_context: &str,
        messages: &[ChatMessage],
        current_message: &str,
    ) -> TokenCount {
        let system_tokens = Self::count_text(system_context);
        let history_tokens = Self::count_messages(messages);
        let current_message_tokens = Self::count_text(current_message);

        TokenCount {
            total: system_tokens + history_tokens + current_message_tokens,
            system_tokens,
            history_tokens,
            current_message_tokens,
        }
    }

    pub fn estimate_total(
        system_approx: usize,
        messages: &[ChatMessage],
        current_message: &str,
    ) -> usize {
        system_approx 
            + Self::count_messages(messages) 
            + Self::count_text(current_message)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_count_text() {
        let text = "Hello world";
        let tokens = TokenCounter::count_text(text);
        assert!(tokens >= 3 && tokens <= 6);
    }

    #[test]
    fn test_count_messages() {
        let messages = vec![
            ChatMessage::user("What is RAG?"),
            ChatMessage::assistant("RAG is Retrieval-Augmented Generation"),
        ];
        let tokens = TokenCounter::count_messages(&messages);
        assert!(tokens > 0);
    }

    #[test]
    fn test_empty_text() {
        assert_eq!(TokenCounter::count_text(""), 0);
    }
}
```

***

### **STEP 4: FIXED `src/services/conversation/context_builder.rs`**

```rust
use anyhow::{Context, Result};
use tracing::{debug, info};
use crate::models::chat::ChatMessage;  // Import from models
use crate::utils::similarity::cosine_similarity;
use super::types::{
    ConversationState, RetrievalDecision, RetrievalReason, 
    SkipReason, SystemContextComponents, WeightedEmbeddingConfig
};

pub struct ContextBuilder {
    base_instruction: String,
    similarity_threshold: f32,
    weighted_config: WeightedEmbeddingConfig,
}

impl ContextBuilder {
    pub fn new(base_instruction: String) -> Self {
        Self {
            base_instruction,
            similarity_threshold: 0.75,
            weighted_config: WeightedEmbeddingConfig::default(),
        }
    }

    pub fn default_base_instruction() -> String {
        r#"You are an intelligent AI assistant for a Document Management System.

Your role is to help users understand and work with their documents by:
- Answering questions based on the provided document context
- Providing accurate information from the retrieved documents
- Being concise and helpful in your responses
- Admitting when information is not available in the context

Guidelines:
- Always base your answers on the provided document context
- If the context doesn't contain relevant information, say so clearly
- Cite document sources when possible
- Be conversational but professional
- Keep responses focused and relevant to the user's question"#.to_string()
    }

    pub fn decide_retrieval(
        &self,
        state: &ConversationState,
        _current_message: &str,
        current_document_id: Option<i64>,
        current_embedding: Option<&Vec<f32>>,
    ) -> Result<RetrievalDecision> {
        if state.messages.is_empty() {
            debug!("First message in session, need retrieval");
            return Ok(RetrievalDecision::Retrieve {
                reason: RetrievalReason::FirstMessage,
                context_aware: false,
            });
        }

        if state.document_id != current_document_id {
            info!(
                "Document ID changed from {:?} to {:?}, need new retrieval",
                state.document_id, current_document_id
            );
            return Ok(RetrievalDecision::Retrieve {
                reason: RetrievalReason::DocumentIdChanged,
                context_aware: true,
            });
        }

        if let (Some(current_emb), Some(last_emb)) = 
            (current_embedding, &state.last_query_embedding) 
        {
            let similarity = cosine_similarity(current_emb, last_emb)
                .context("Failed to calculate similarity")?;

            debug!("Similarity with last query: {:.4}", similarity);

            if similarity > self.similarity_threshold {
                info!(
                    "High similarity ({:.4} > {}), skipping retrieval",
                    similarity, self.similarity_threshold
                );
                return Ok(RetrievalDecision::Skip {
                    reason: SkipReason::SameDocumentAndHighSimilarity(similarity),
                });
            } else {
                info!(
                    "Low similarity ({:.4} <= {}), need new retrieval",
                    similarity, self.similarity_threshold
                );
                return Ok(RetrievalDecision::Retrieve {
                    reason: RetrievalReason::LowSimilarity(similarity),
                    context_aware: true,
                });
            }
        }

        debug!("No previous embedding found, performing retrieval");
        Ok(RetrievalDecision::Retrieve {
            reason: RetrievalReason::FirstMessage,
            context_aware: false,
        })
    }

    pub fn build_system_context(
        &self,
        retrieval_summary: &str,
        document_metadata: Option<&str>,
    ) -> String {
        let components = SystemContextComponents {
            base_instruction: self.base_instruction.clone(),
            retrieval_context: retrieval_summary.to_string(),
            metadata_section: document_metadata.map(|s| s.to_string()),
        };

        components.build()
    }

    pub fn prepare_context_aware_text(
        &self,
        current_message: &str,
        history: &[ChatMessage],
    ) -> String {
        if history.is_empty() {
            return current_message.to_string();
        }

        let last_user_messages: Vec<String> = history
            .iter()
            .filter(|msg| msg.role == "user")
            .rev()
            .take(self.weighted_config.max_history_messages)
            .map(|msg| msg.content.clone())
            .collect::<Vec<_>>()
            .into_iter()
            .rev()
            .collect();

        if last_user_messages.is_empty() {
            return current_message.to_string();
        }

        let history_text = last_user_messages.join(" ");
        format!("{} {}", history_text, current_message)
    }

    pub fn weighted_config(&self) -> &WeightedEmbeddingConfig {
        &self.weighted_config
    }
}

impl Default for ContextBuilder {
    fn default() -> Self {
        Self::new(Self::default_base_instruction())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::chat::SessionId;

    #[test]
    fn test_decide_retrieval_first_message() {
        let builder = ContextBuilder::default();
        let state = ConversationState::new(123 as SessionId, 456, None);
        
        let decision = builder.decide_retrieval(&state, "test", None, None).unwrap();
        
        match decision {
            RetrievalDecision::Retrieve { reason, context_aware } => {
                assert!(matches!(reason, RetrievalReason::FirstMessage));
                assert!(!context_aware);
            }
            _ => panic!("Expected Retrieve decision"),
        }
    }
}
```

***

### **STEP 5: FIXED `src/services/conversation/manager.rs`**

```rust
use anyhow::{Context, Result};
use tracing::{debug, error, info, warn};
use crate::models::chat::ChatMessage;  // Import from models, not from types
use super::cache::ConversationCache;
use super::context_builder::ContextBuilder;
use super::token_counter::TokenCounter;
use super::types::{
    ConversationState, RetrievalDecision, SessionId, TokenCount,
};

// Remove these imports - we'll use trait objects instead
// use crate::services::embedding::EmbeddingService;
// use crate::services::llm::LlmService;
// use crate::services::retrieval::RetrievalService;

/// Trait for embedding service (break circular dependency)
#[async_trait::async_trait]
pub trait EmbeddingProvider: Send + Sync {
    async fn embed(&self, text: &str) -> Result<Vec<f32>>;
    async fn embed_weighted(
        &self,
        current_text: &str,
        context_text: &str,
        current_weight: f32,
        history_weight: f32,
    ) -> Result<Vec<f32>>;
}

/// Trait for retrieval service
#[async_trait::async_trait]
pub trait RetrievalProvider: Send + Sync {
    async fn search(
        &self,
        user_id: i64,
        embedding: &[f32],
        document_id: Option<i64>,
    ) -> Result<Vec<RetrievalChunk>>;
}

/// Trait for LLM service
#[async_trait::async_trait]
pub trait LlmProvider: Send + Sync {
    async fn generate(&self, messages: &[ChatMessage]) -> Result<String>;
    async fn summarize_chunks(&self, chunks: &[RetrievalChunk]) -> Result<String>;
}

/// Chunk result from retrieval (define here to avoid circular dep)
#[derive(Debug, Clone)]
pub struct RetrievalChunk {
    pub chunk_id: i64,
    pub document_id: i64,
    pub document_title: Option<String>,
    pub content: String,
    pub similarity: f32,
}

pub struct ConversationManager {
    cache: ConversationCache,
    context_builder: ContextBuilder,
    embedding_provider: Box<dyn EmbeddingProvider>,
    retrieval_provider: Box<dyn RetrievalProvider>,
    llm_provider: Box<dyn LlmProvider>,
}

impl ConversationManager {
    pub fn new(
        embedding_provider: Box<dyn EmbeddingProvider>,
        retrieval_provider: Box<dyn RetrievalProvider>,
        llm_provider: Box<dyn LlmProvider>,
    ) -> Self {
        Self {
            cache: ConversationCache::new(),
            context_builder: ContextBuilder::default(),
            embedding_provider,
            retrieval_provider,
            llm_provider,
        }
    }

    pub fn generate_session_id(user_id: i64) -> SessionId {
        let now = chrono::Utc::now();
        let timestamp = now.format("%Y%m%d%H%M%S").to_string();
        format!("{}{}", timestamp, user_id)
            .parse()
            .expect("Failed to parse session_id")
    }

    pub async fn get_or_create_session(
        &self,
        session_id: SessionId,
        user_id: i64,
        document_id: Option<i64>,
    ) -> Result<ConversationState> {
        if let Some(state) = self.cache.get(session_id) {
            debug!("Found existing session {}", session_id);
            return Ok(state);
        }

        if !self.cache.can_create_new_session() {
            anyhow::bail!("Memory limit reached (90%), cannot create new session");
        }

        info!("Creating new session {} for user {}", session_id, user_id);
        let state = ConversationState::new(session_id, user_id, document_id);
        self.cache.set(session_id, state.clone());

        Ok(state)
    }

    pub async fn handle_message(
        &self,
        session_id: SessionId,
        user_id: i64,
        message: String,
        document_id: Option<i64>,
    ) -> Result<String> {
        info!("Handling message for session {}, user {}", session_id, user_id);

        let mut state = self.get_or_create_session(session_id, user_id, document_id).await?;

        self.enforce_sliding_window(&mut state)?;

        let current_embedding = self.embedding_provider
            .embed(&message)
            .await
            .context("Failed to embed current message")?;

        let decision = self.context_builder.decide_retrieval(
            &state,
            &message,
            document_id,
            Some(&current_embedding),
        )?;

        let system_context = self.execute_retrieval_decision(
            &mut state,
            &decision,
            &message,
            document_id,
            &current_embedding,
        ).await?;

        state.messages.push(ChatMessage::user(&message));

        self.manage_tokens(&mut state, &system_context).await?;

        let llm_messages = self.prepare_llm_payload(&state, &system_context);

        let assistant_response = self.call_llm_with_retry(&llm_messages).await?;

        state.messages.push(ChatMessage::assistant(&assistant_response));
        state.last_query_embedding = Some(current_embedding);
        state.metadata.total_messages += 2;
        state.touch();

        self.cache.set(session_id, state);

        Ok(assistant_response)
    }

    fn enforce_sliding_window(&self, state: &mut ConversationState) -> Result<()> {
        if !state.needs_window_enforcement() {
            return Ok(());
        }

        info!(
            "Enforcing sliding window for session {} (current pairs: {})",
            state.session_id,
            state.message_pair_count()
        );

        if state.messages.len() >= 2 {
            state.messages.drain(0..2);
            debug!("Removed oldest message pair (Q1, A1)");
        }

        Ok(())
    }

    async fn execute_retrieval_decision(
        &self,
        state: &mut ConversationState,
        decision: &RetrievalDecision,
        current_message: &str,
        document_id: Option<i64>,
        current_embedding: &[f32],
    ) -> Result<String> {
        match decision {
            RetrievalDecision::Skip { reason } => {
                debug!("Skipping retrieval: {:?}", reason);
                state.metadata.retrieval_skipped_count += 1;
                Ok(state.system_context.clone())
            }
            RetrievalDecision::Retrieve { reason, context_aware } => {
                info!("Performing retrieval: {:?}", reason);
                state.metadata.total_retrievals += 1;

                let query_embedding = if *context_aware {
                    let context_text = self.context_builder
                        .prepare_context_aware_text(current_message, &state.messages);
                    
                    let config = self.context_builder.weighted_config();
                    self.embedding_provider
                        .embed_weighted(
                            current_message,
                            &context_text,
                            config.current_weight,
                            config.history_weight,
                        )
                        .await?
                } else {
                    current_embedding.to_vec()
                };

                let chunks = self.retrieval_provider
                    .search(state.user_id, &query_embedding, document_id)
                    .await
                    .context("Retrieval failed")?;

                let summary = self.llm_provider
                    .summarize_chunks(&chunks)
                    .await
                    .context("Failed to summarize chunks")?;

                let system_context = self.context_builder.build_system_context(
                    &summary,
                    document_id.map(|id| format!("Document ID: {}", id)).as_deref(),
                );

                state.system_context = system_context.clone();
                state.last_retrieval_summary = summary;
                state.document_id = document_id;

                Ok(system_context)
            }
        }
    }

    async fn manage_tokens(
        &self,
        state: &mut ConversationState,
        system_context: &str,
    ) -> Result<()> {
        let token_count = TokenCounter::count_payload(
            system_context,
            &state.messages,
            "",
        );

        debug!("Token count: {} (system: {}, history: {})", 
            token_count.total, token_count.system_tokens, token_count.history_tokens);

        state.metadata.total_tokens_last = token_count.total;

        if !token_count.is_over_soft_limit() {
            return Ok(());
        }

        warn!("Token count {} exceeds 20K, performing cascade deletion", token_count.total);

        let mut current_count = token_count.total;
        let mut deletion_round = 1;

        while current_count > 20_000 && state.messages.len() >= 2 {
            info!("Deletion round {}: removing oldest pair", deletion_round);
            
            state.messages.drain(0..2);
            
            let new_count = TokenCounter::count_payload(
                system_context,
                &state.messages,
                "",
            );
            current_count = new_count.total;
            
            debug!("After deletion round {}: {} tokens", deletion_round, current_count);
            deletion_round += 1;

            if state.messages.is_empty() {
                warn!("All history deleted, only current message remains");
                break;
            }
        }

        if current_count > 23_000 {
            warn!("Token count {} still over 23K after deletion, truncating retrieval", current_count);
            
            let truncated_summary = state.last_retrieval_summary
                .chars()
                .take(500)
                .collect::<String>() + "... (truncated)";
            
            let new_system = self.context_builder.build_system_context(
                &truncated_summary,
                state.document_id.map(|id| format!("Document ID: {}", id)).as_deref(),
            );
            
            state.system_context = new_system;
            
            info!("Retrieval context truncated");
        }

        Ok(())
    }

    fn prepare_llm_payload(
        &self,
        state: &ConversationState,
        system_context: &str,
    ) -> Vec<ChatMessage> {
        let mut messages = vec![ChatMessage::system(system_context)];
        messages.extend(state.messages.clone());
        messages
    }

    async fn call_llm_with_retry(&self, messages: &[ChatMessage]) -> Result<String> {
        const MAX_RETRIES: u32 = 3;
        
        for attempt in 1..=MAX_RETRIES {
            match self.llm_provider.generate(messages).await {
                Ok(response) => {
                    debug!("LLM call succeeded on attempt {}", attempt);
                    return Ok(response);
                }
                Err(e) => {
                    if attempt < MAX_RETRIES {
                        warn!("LLM call failed (attempt {}): {}. Retrying...", attempt, e);
                        tokio::time::sleep(tokio::time::Duration::from_secs(attempt as u64)).await;
                    } else {
                        error!("LLM call failed after {} attempts: {}", MAX_RETRIES, e);
                        anyhow::bail!("Server ada gangguan, silakan coba lagi nanti");
                    }
                }
            }
        }

        unreachable!()
    }

    pub fn cache_stats(&self) -> super::cache::CacheStats {
        self.cache.stats()
    }

    pub fn cleanup_expired_sessions(&self) -> usize {
        self.cache.cleanup_expired()
    }
}
```

***
