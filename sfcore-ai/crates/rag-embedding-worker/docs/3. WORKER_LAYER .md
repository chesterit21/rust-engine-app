Siap bro! Lanjut ke bagian **Worker Layer** dan **Utils** yang merupakan inti dari aplikasi ini:

***

### ‚öôÔ∏è WORKER LAYER (Main Processing Logic)

**`src/worker/mod.rs`**

```rust
pub mod processor;
pub mod bulk_indexer;
pub mod queue;

pub use processor::DocumentProcessor;
pub use bulk_indexer::BulkIndexer;
pub use queue::{TaskQueue, Task, TaskPriority};

use crate::config::Settings;
use crate::database::{DbPool, NotificationListener, Repository};
use crate::embedding::LlamaServerManager;
use anyhow::Result;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{error, info, warn};

pub struct Worker {
    settings: Settings,
    repository: Arc<Repository>,
    listener: NotificationListener,
    task_queue: Arc<TaskQueue>,
    llama_manager: Arc<RwLock<LlamaServerManager>>,
    processor: Arc<DocumentProcessor>,
}

impl Worker {
    pub async fn new(settings: Settings, db_pool: DbPool) -> Result<Self> {
        let repository = Arc::new(Repository::new(db_pool.clone()));
        
        let listener = NotificationListener::new(
            db_pool.clone(),
            settings.database.listen_channel.clone(),
        );
        
        let task_queue = Arc::new(TaskQueue::new(settings.worker.bulk_batch_size));
        
        let llama_manager = Arc::new(RwLock::new(LlamaServerManager::new(
            settings.llama_server.clone(),
        )));
        
        let processor = Arc::new(DocumentProcessor::new(
            settings.clone(),
            repository.clone(),
            llama_manager.clone(),
        ));
        
        Ok(Self {
            settings,
            repository,
            listener,
            task_queue,
            llama_manager,
            processor,
        })
    }
    
    /// Main worker loop
    pub async fn run(self) -> Result<()> {
        info!("üéØ Worker started");
        
        // Start notification listener
        let mut notification_rx = self.listener.start().await?;
        
        // Spawn task processor
        let processor_handle = {
            let task_queue = self.task_queue.clone();
            let processor = self.processor.clone();
            
            tokio::spawn(async move {
                loop {
                    // Get next task from queue
                    if let Some(task) = task_queue.dequeue().await {
                        info!("Processing task: document_id={}", task.document_id);
                        
                        match processor.process_document(task.document_id).await {
                            Ok(_) => {
                                info!("‚úÖ Successfully processed document {}", task.document_id);
                            }
                            Err(e) => {
                                error!("‚ùå Failed to process document {}: {}", task.document_id, e);
                            }
                        }
                    } else {
                        // No tasks, sleep a bit
                        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                    }
                }
            })
        };
        
        // Handle initial bulk indexing if needed
        info!("üîç Checking for unprocessed documents...");
        match self.check_and_run_bulk_indexing().await {
            Ok(count) => {
                if count > 0 {
                    info!("üì¶ Bulk indexing completed: {} documents", count);
                }
            }
            Err(e) => {
                error!("Failed to run bulk indexing: {}", e);
            }
        }
        
        // Listen for notifications
        info!("üëÇ Listening for document changes...");
        loop {
            tokio::select! {
                // Handle notifications
                Some(notification) = notification_rx.recv() => {
                    info!(
                        "üì¨ Received notification: op={}, doc_id={}",
                        notification.operation,
                        notification.document_id
                    );
                    
                    // Enqueue task
                    self.task_queue.enqueue(Task {
                        document_id: notification.document_id,
                        priority: TaskPriority::Normal,
                        retry_count: 0,
                    }).await;
                }
                
                // Graceful shutdown signal
                _ = tokio::signal::ctrl_c() => {
                    info!("Received shutdown signal");
                    break;
                }
            }
        }
        
        // Cleanup
        info!("Shutting down worker...");
        processor_handle.abort();
        
        // Stop llama-server if running
        let mut llama = self.llama_manager.write().await;
        llama.stop().await?;
        
        info!("Worker stopped");
        Ok(())
    }
    
    /// Check for unprocessed documents and run bulk indexing if needed
    async fn check_and_run_bulk_indexing(&self) -> Result<usize> {
        let bulk_indexer = BulkIndexer::new(
            self.settings.clone(),
            self.repository.clone(),
            self.llama_manager.clone(),
        );
        
        bulk_indexer.run().await
    }
}
```

***

**`src/worker/processor.rs`**

```rust
use crate::config::Settings;
use crate::database::{DocumentChunk, IngestionLog, IngestionStatus, Repository};
use crate::document::{DocumentLoader, DocumentParser, TextChunker};
use crate::embedding::{EmbeddingProvider, EmbeddingRequest, LlamaServerManager};
use crate::utils::error::WorkerError;
use anyhow::Result;
use chrono::Utc;
use pgvector::Vector;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

pub struct DocumentProcessor {
    settings: Settings,
    repository: Arc<Repository>,
    llama_manager: Arc<RwLock<LlamaServerManager>>,
}

impl DocumentProcessor {
    pub fn new(
        settings: Settings,
        repository: Arc<Repository>,
        llama_manager: Arc<RwLock<LlamaServerManager>>,
    ) -> Self {
        Self {
            settings,
            repository,
            llama_manager,
        }
    }
    
    /// Process single document
    pub async fn process_document(&self, document_id: i32) -> Result<()> {
        info!("üìÑ Processing document {}", document_id);
        
        // Get document info from database
        let doc_file = self.repository.get_document_file(document_id).await?
            .ok_or_else(|| WorkerError::DocumentNotFound(document_id))?;
        
        let file_path = self.resolve_file_path(&doc_file.document_file_path)?;
        
        // Validate file
        DocumentLoader::validate_file(&file_path, 100)?; // max 100MB
        
        // Create or update ingestion log
        self.create_ingestion_log(document_id, &file_path).await?;
        
        // Update status to processing
        self.repository
            .update_ingestion_status(document_id, IngestionStatus::Processing, None)
            .await?;
        
        // Process document
        match self.process_document_internal(document_id, &file_path).await {
            Ok(_) => {
                // Update status to completed
                self.repository
                    .update_ingestion_status(document_id, IngestionStatus::Completed, None)
                    .await?;
                
                info!("‚úÖ Document {} processed successfully", document_id);
                Ok(())
            }
            Err(e) => {
                error!("‚ùå Failed to process document {}: {}", document_id, e);
                
                // Update status to failed
                self.repository
                    .update_ingestion_status(
                        document_id,
                        IngestionStatus::Failed,
                        Some(e.to_string()),
                    )
                    .await?;
                
                Err(e)
            }
        }
    }
    
    /// Internal processing logic
    async fn process_document_internal(
        &self,
        document_id: i32,
        file_path: &PathBuf,
    ) -> Result<()> {
        // 1. Parse document
        info!("üìñ Parsing document...");
        let parsed = DocumentParser::parse(file_path)?;
        
        if parsed.content.trim().is_empty() {
            warn!("Document {} has no extractable text", document_id);
            return Ok(());
        }
        
        // 2. Chunk text
        info!("‚úÇÔ∏è  Chunking text...");
        let chunker = TextChunker::new(
            self.settings.chunking.size,
            self.settings.chunking.overlap,
            self.settings.chunking.strategy.clone(),
        );
        
        let chunks = chunker.chunk(&parsed.content)?;
        
        if chunks.is_empty() {
            warn!("Document {} produced no chunks", document_id);
            return Ok(());
        }
        
        info!("Created {} chunks for document {}", chunks.len(), document_id);
        
        // 3. Generate embeddings (start llama-server on-demand)
        info!("üß† Generating embeddings...");
        
        // Start llama-server
        {
            let mut llama = self.llama_manager.write().await;
            llama.start().await?;
        }
        
        // Generate embeddings
        let embeddings = {
            let llama = self.llama_manager.read().await;
            let texts: Vec<String> = chunks.iter().map(|c| c.content.clone()).collect();
            
            let request = EmbeddingRequest { texts };
            let response = llama.embed(request).await?;
            
            response.embeddings
        };
        
        // Stop llama-server to free memory
        {
            let mut llama = self.llama_manager.write().await;
            llama.stop().await?;
        }
        
        info!("‚úÖ Generated {} embeddings", embeddings.len());
        
        // 4. Delete existing chunks (for re-indexing)
        let deleted = self.repository.delete_chunks_by_document(document_id).await?;
        if deleted > 0 {
            info!("Deleted {} existing chunks", deleted);
        }
        
        // 5. Save chunks to database
        info!("üíæ Saving chunks to database...");
        
        let db_chunks: Vec<DocumentChunk> = chunks
            .into_iter()
            .zip(embeddings.into_iter())
            .map(|(chunk, embedding)| DocumentChunk {
                document_id,
                tenant_id: None, // TODO: extract from document if needed
                chunk_index: chunk.index as i32,
                content: chunk.content,
                char_count: chunk.char_count as i32,
                token_count: chunk.token_count.map(|t| t as i32),
                embedding: Vector::from(embedding),
                page_number: None, // TODO: extract page info if available
                section: None,
                tags: None,
            })
            .collect();
        
        self.repository.insert_chunks(db_chunks).await?;
        
        info!("‚úÖ Saved chunks to database");
        
        Ok(())
    }
    
    /// Resolve file path (handle relative paths)
    fn resolve_file_path(&self, path_str: &str) -> Result<PathBuf> {
        let path = PathBuf::from(path_str);
        
        if path.is_absolute() {
            Ok(path)
        } else {
            // Relative to document root
            let full_path = self.settings.worker.document_root_path.join(path);
            Ok(full_path)
        }
    }
    
    /// Create ingestion log entry
    async fn create_ingestion_log(
        &self,
        document_id: i32,
        file_path: &PathBuf,
    ) -> Result<()> {
        let file_type = DocumentLoader::detect_file_type(file_path).ok();
        let file_size = std::fs::metadata(file_path).ok().map(|m| m.len() as i64);
        
        let log = IngestionLog {
            document_id,
            file_path: file_path.to_string_lossy().to_string(),
            file_size,
            file_type,
            embedding_model: self.settings.embedding.model.clone(),
            chunk_size: self.settings.chunking.size as i32,
            chunk_overlap: self.settings.chunking.overlap as i32,
            status: IngestionStatus::Pending.to_string(),
            total_chunks: 0,
            processed_chunks: 0,
            last_error: None,
            retry_count: 0,
            started_at: Some(Utc::now()),
            processed_at: None,
        };
        
        self.repository.upsert_ingestion_log(&log).await?;
        
        Ok(())
    }
}
```

***

**`src/worker/bulk_indexer.rs`**

```rust
use crate::config::Settings;
use crate::database::{IngestionStatus, Repository};
use crate::embedding::LlamaServerManager;
use crate::worker::DocumentProcessor;
use anyhow::Result;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{error, info, warn};

pub struct BulkIndexer {
    settings: Settings,
    repository: Arc<Repository>,
    llama_manager: Arc<RwLock<LlamaServerManager>>,
}

impl BulkIndexer {
    pub fn new(
        settings: Settings,
        repository: Arc<Repository>,
        llama_manager: Arc<RwLock<LlamaServerManager>>,
    ) -> Self {
        Self {
            settings,
            repository,
            llama_manager,
        }
    }
    
    /// Run bulk indexing for all unprocessed documents
    pub async fn run(&self) -> Result<usize> {
        info!("üöÄ Starting bulk indexing...");
        
        // Get all documents
        let all_docs = self.repository.get_all_document_files().await?;
        info!("Found {} total documents", all_docs.len());
        
        // Filter unprocessed documents
        let mut unprocessed = Vec::new();
        
        for doc in all_docs {
            // Check if already processed
            match self.repository.get_ingestion_log(doc.document_id).await? {
                Some(log) => {
                    // Re-index if failed
                    if log.status == IngestionStatus::Failed.to_string() {
                        info!("Document {} previously failed, will retry", doc.document_id);
                        unprocessed.push(doc);
                    } else if log.status != IngestionStatus::Completed.to_string() {
                        unprocessed.push(doc);
                    }
                }
                None => {
                    // Never processed
                    unprocessed.push(doc);
                }
            }
        }
        
        if unprocessed.is_empty() {
            info!("‚úÖ All documents already processed");
            return Ok(0);
        }
        
        info!("üì¶ Found {} unprocessed documents", unprocessed.len());
        
        // Create processor
        let processor = DocumentProcessor::new(
            self.settings.clone(),
            self.repository.clone(),
            self.llama_manager.clone(),
        );
        
        // Process in batches
        let batch_size = self.settings.worker.bulk_batch_size;
        let total = unprocessed.len();
        let mut processed_count = 0;
        let mut success_count = 0;
        
        for (batch_idx, batch) in unprocessed.chunks(batch_size).enumerate() {
            info!(
                "Processing batch {}/{} ({} documents)",
                batch_idx + 1,
                (total + batch_size - 1) / batch_size,
                batch.len()
            );
            
            // Start llama-server once per batch
            {
                let mut llama = self.llama_manager.write().await;
                match llama.start().await {
                    Ok(_) => info!("‚úÖ Llama-server started for batch"),
                    Err(e) => {
                        error!("Failed to start llama-server: {}", e);
                        continue;
                    }
                }
            }
            
            // Process documents in batch sequentially
            for doc in batch {
                processed_count += 1;
                
                info!(
                    "Processing document {}/{}: id={}",
                    processed_count, total, doc.document_id
                );
                
                match processor.process_document(doc.document_id).await {
                    Ok(_) => {
                        success_count += 1;
                        info!("‚úÖ Successfully processed document {}", doc.document_id);
                    }
                    Err(e) => {
                        error!("‚ùå Failed to process document {}: {}", doc.document_id, e);
                    }
                }
            }
            
            // Stop llama-server to free memory
            {
                let mut llama = self.llama_manager.write().await;
                if let Err(e) = llama.stop().await {
                    warn!("Failed to stop llama-server: {}", e);
                }
            }
            
            info!("Batch {} completed, waiting before next batch...", batch_idx + 1);
            tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
        }
        
        info!(
            "üéâ Bulk indexing completed: {}/{} documents successful",
            success_count, total
        );
        
        Ok(success_count)
    }
}
```

***

**`src/worker/queue.rs`**

```rust
use std::collections::VecDeque;
use tokio::sync::Mutex;

#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum TaskPriority {
    Low = 0,
    Normal = 1,
    High = 2,
}

#[derive(Debug, Clone)]
pub struct Task {
    pub document_id: i32,
    pub priority: TaskPriority,
    pub retry_count: u32,
}

pub struct TaskQueue {
    queue: Mutex<VecDeque<Task>>,
    max_size: usize,
}

impl TaskQueue {
    pub fn new(max_size: usize) -> Self {
        Self {
            queue: Mutex::new(VecDeque::new()),
            max_size,
        }
    }
    
    /// Enqueue task (sorted by priority)
    pub async fn enqueue(&self, task: Task) {
        let mut queue = self.queue.lock().await;
        
        // Check if document already in queue
        if queue.iter().any(|t| t.document_id == task.document_id) {
            return; // Skip duplicate
        }
        
        // Check max size
        if queue.len() >= self.max_size {
            // Remove lowest priority task
            if let Some(pos) = queue.iter().position(|t| t.priority == TaskPriority::Low) {
                queue.remove(pos);
            }
        }
        
        // Insert based on priority
        let insert_pos = queue
            .iter()
            .position(|t| t.priority < task.priority)
            .unwrap_or(queue.len());
        
        queue.insert(insert_pos, task);
    }
    
    /// Dequeue next task
    pub async fn dequeue(&self) -> Option<Task> {
        let mut queue = self.queue.lock().await;
        queue.pop_front()
    }
    
    /// Get queue size
    pub async fn size(&self) -> usize {
        let queue = self.queue.lock().await;
        queue.len()
    }
}
```

***

### üõ†Ô∏è UTILS LAYER (Error Handling, Logging, Metrics)

**`src/utils/mod.rs`**

```rust
pub mod error;
pub mod logger;
pub mod metrics;

pub use error::WorkerError;
```

***

**`src/utils/error.rs`**

```rust
use thiserror::Error;

#[derive(Error, Debug)]
pub enum WorkerError {
    #[error("Document not found: {0}")]
    DocumentNotFound(i32),
    
    #[error("File not found: {0}")]
    FileNotFound(String),
    
    #[error("Unsupported file type: {0}")]
    UnsupportedFileType(String),
    
    #[error("File too large: {0} MB (max: {1} MB)")]
    FileTooLarge(u64, u64),
    
    #[error("Parsing error: {0}")]
    ParsingError(String),
    
    #[error("Chunking error: {0}")]
    ChunkingError(String),
    
    #[error("Embedding error: {0}")]
    EmbeddingError(String),
    
    #[error("Database error: {0}")]
    DatabaseError(#[from] sqlx::Error),
    
    #[error("IO error: {0}")]
    IoError(#[from] std::io::Error),
    
    #[error("HTTP error: {0}")]
    HttpError(#[from] reqwest::Error),
    
    #[error("Llama-server not running")]
    LlamaServerNotRunning,
    
    #[error("Llama-server failed to start: {0}")]
    LlamaServerStartFailed(String),
    
    #[error("Insufficient memory: {0} MB available (required: {1} MB)")]
    InsufficientMemory(u64, u64),
    
    #[error("Configuration error: {0}")]
    ConfigError(String),
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

impl From<anyhow::Error> for WorkerError {
    fn from(err: anyhow::Error) -> Self {
        WorkerError::Unknown(err.to_string())
    }
}
```

***

**`src/utils/logger.rs`**

```rust
use anyhow::Result;
use tracing::Level;
use tracing_appender::rolling::{RollingFileAppender, Rotation};
use tracing_subscriber::{
    fmt,
    layer::SubscriberExt,
    util::SubscriberInitExt,
    EnvFilter,
};

pub fn init_logger() -> Result<()> {
    // Get log level from environment (default: info)
    let log_level = std::env::var("RUST_LOG")
        .unwrap_or_else(|_| "info,rag_embedding_worker=debug".to_string());
    
    // Get log format from environment (default: pretty)
    let log_format = std::env::var("LOG_FORMAT")
        .unwrap_or_else(|_| "pretty".to_string());
    
    // Create file appender (logs/app.log, daily rotation)
    let file_appender = RollingFileAppender::builder()
        .rotation(Rotation::DAILY)
        .filename_prefix("app")
        .filename_suffix("log")
        .build("logs")?;
    
    // Create filter
    let filter = EnvFilter::try_new(&log_level)?;
    
    // Setup subscriber
    match log_format.as_str() {
        "json" => {
            // JSON format untuk production
            tracing_subscriber::registry()
                .with(filter)
                .with(
                    fmt::layer()
                        .json()
                        .with_writer(std::io::stdout)
                        .with_target(true)
                        .with_level(true)
                        .with_thread_ids(true)
                )
                .with(
                    fmt::layer()
                        .json()
                        .with_writer(file_appender)
                        .with_target(true)
                        .with_level(true)
                        .with_thread_ids(true)
                )
                .init();
        }
        _ => {
            // Pretty format untuk development
            tracing_subscriber::registry()
                .with(filter)
                .with(
                    fmt::layer()
                        .pretty()
                        .with_writer(std::io::stdout)
                        .with_target(true)
                        .with_level(true)
                        .with_thread_ids(false)
                )
                .with(
                    fmt::layer()
                        .with_writer(file_appender)
                        .with_target(true)
                        .with_level(true)
                        .with_ansi(false) // No colors in file
                )
                .init();
        }
    }
    
    Ok(())
}
```

***

**`src/utils/metrics.rs`**

```rust
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};

#[derive(Clone)]
pub struct Metrics {
    inner: Arc<MetricsInner>,
}

struct MetricsInner {
    documents_processed: AtomicU64,
    documents_failed: AtomicU64,
    chunks_created: AtomicU64,
    total_processing_time_ms: AtomicU64,
    embeddings_generated: AtomicU64,
}

impl Metrics {
    pub fn new() -> Self {
        Self {
            inner: Arc::new(MetricsInner {
                documents_processed: AtomicU64::new(0),
                documents_failed: AtomicU64::new(0),
                chunks_created: AtomicU64::new(0),
                total_processing_time_ms: AtomicU64::new(0),
                embeddings_generated: AtomicU64::new(0),
            }),
        }
    }
    
    pub fn increment_documents_processed(&self) {
        self.inner.documents_processed.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn increment_documents_failed(&self) {
        self.inner.documents_failed.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn add_chunks_created(&self, count: u64) {
        self.inner.chunks_created.fetch_add(count, Ordering::Relaxed);
    }
    
    pub fn add_processing_time(&self, duration: Duration) {
        self.inner
            .total_processing_time_ms
            .fetch_add(duration.as_millis() as u64, Ordering::Relaxed);
    }
    
    pub fn add_embeddings_generated(&self, count: u64) {
        self.inner.embeddings_generated.fetch_add(count, Ordering::Relaxed);
    }
    
    pub fn get_documents_processed(&self) -> u64 {
        self.inner.documents_processed.load(Ordering::Relaxed)
    }
    
    pub fn get_documents_failed(&self) -> u64 {
        self.inner.documents_failed.load(Ordering::Relaxed)
    }
    
    pub fn get_chunks_created(&self) -> u64 {
        self.inner.chunks_created.load(Ordering::Relaxed)
    }
    
    pub fn get_total_processing_time_ms(&self) -> u64 {
        self.inner.total_processing_time_ms.load(Ordering::Relaxed)
    }
    
    pub fn get_embeddings_generated(&self) -> u64 {
        self.inner.embeddings_generated.load(Ordering::Relaxed)
    }
    
    pub fn get_average_processing_time_ms(&self) -> f64 {
        let processed = self.get_documents_processed();
        if processed == 0 {
            return 0.0;
        }
        
        let total_time = self.get_total_processing_time_ms();
        total_time as f64 / processed as f64
    }
    
    pub fn print_summary(&self) {
        println!("\nüìä === METRICS SUMMARY ===");
        println!("Documents Processed: {}", self.get_documents_processed());
        println!("Documents Failed: {}", self.get_documents_failed());
        println!("Chunks Created: {}", self.get_chunks_created());
        println!("Embeddings Generated: {}", self.get_embeddings_generated());
        println!(
            "Average Processing Time: {:.2}ms",
            self.get_average_processing_time_ms()
        );
        println!(
            "Total Processing Time: {:.2}s",
            self.get_total_processing_time_ms() as f64 / 1000.0
        );
        println!("=========================\n");
    }
}

impl Default for Metrics {
    fn default() -> Self {
        Self::new()
    }
}

/// Timer helper untuk measure duration
pub struct Timer {
    start: Instant,
}

impl Timer {
    pub fn new() -> Self {
        Self {
            start: Instant::now(),
        }
    }
    
    pub fn elapsed(&self) -> Duration {
        self.start.elapsed()
    }
}

impl Default for Timer {
    fn default() -> Self {
        Self::new()
    }
}
```

***

### üìÑ UPDATED CONFIG FILE

**`config/settings.toml`**

```toml
[database]
url = "postgres://user:password@localhost:5432/dbname"
pool_max_size = 10
pool_timeout_seconds = 30
listen_channel = "document_changes"

[embedding]
model = "AllMiniLML6V2"  # Model name
dimension = 384  # 384 for AllMiniLML6V2, 1536 for text-embedding-3-small
batch_size = 32

[chunking]
size = 512
overlap = 50
strategy = "semantic"  # semantic, fixed, recursive

[worker]
threads = 4
bulk_batch_size = 10  # Process 10 docs per batch
processing_timeout_seconds = 300
document_root_path = "/path/to/documents"

[llama_server]
binary_path = "/path/to/llama-server"  # atau llama-server.exe di Windows
model_path = "/path/to/models/all-MiniLM-L6-v2-Q4_K_M.gguf"
host = "127.0.0.1"
port = 8080
startup_timeout_seconds = 60
shutdown_timeout_seconds = 10
embedding_only = true
ctx_size = 2048
threads = 4
```

**`.env.example`** (updated)

```env
# Database
DATABASE_URL=postgres://user:password@localhost:5432/dbname

# Llama Server
LLAMA_SERVER_BINARY=/path/to/llama-server
LLAMA_SERVER_MODEL=/path/to/models/all-MiniLM-L6-v2-Q4_K_M.gguf

# Worker
DOCUMENT_ROOT_PATH=/path/to/documents

# Logging
RUST_LOG=info,rag_embedding_worker=debug
LOG_FORMAT=pretty  # or "json" for production
```

***

Bro, sekarang aplikasi nya sudah lengkap! Mau saya buatkan:

1. **README.md** dengan dokumentasi lengkap
2. **Makefile** untuk build & deployment
3. **systemd service file** untuk auto-start
4. **Docker support** (optional)
5. **Diagram arsitektur & flow** menggunakan Mermaid

Atau ada yang perlu di-adjust dulu di code yang sudah saya berikan? üöÄ
