Perfect bro! Sekarang saya punya semua info yang dibutuhkan. Ini solusi lengkap production-grade untuk RAG Embedding Worker dengan **on-demand llama-server spawning** untuk efisiensi memory!

## ðŸš€ SOLUSI LENGKAP: MEMORY-EFFICIENT RAG EMBEDDING WORKER

### ðŸ“¦ 2. UPDATED CARGO.TOML (Optimized untuk Low RAM)

```toml
[package]
name = "rag-embedding-worker"
version = "0.1.0"
edition = "2021"

[dependencies]
# Async runtime
tokio = { version = "1.42", features = ["full"] }
tokio-postgres = { version = "0.7", features = ["with-chrono-0_4"] }

# Database (pilih salah satu: sqlx atau deadpool-postgres)
sqlx = { version = "0.8", features = [
    "runtime-tokio-native-tls",
    "postgres",
    "chrono",
    "json"
] }
pgvector = { version = "0.4", features = ["sqlx"] }

# HTTP Client untuk llama-server
reqwest = { version = "0.12", features = ["json", "stream"] }

# Document parsing
lopdf = "0.35"  # PDF
docx-rs = "0.4"  # DOCX
pulldown-cmark = "0.12"  # Markdown
scraper = "0.21"  # HTML
encoding_rs = "0.8"  # Text encoding detection

# Text processing & chunking
text-splitter = { version = "0.18", features = ["tiktoken-rs"] }
unicode-segmentation = "1.12"
regex = "1.11"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
toml = "0.8"

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }
tracing-appender = "0.2"

# Configuration
config = "0.14"
dotenvy = "0.15"

# Utilities
chrono = { version = "0.4", features = ["serde"] }
futures = "0.3"
async-trait = "0.1"
once_cell = "1.20"
walkdir = "2.5"
mime_guess = "2.0"
sha2 = "0.10"  # untuk hash checking
bytes = "1.9"

# Process management
sysinfo = "0.33"  # untuk check memory

[dev-dependencies]
mockall = "0.13"
tokio-test = "0.4"
tempfile = "3.14"

[profile.release]
opt-level = 3
lto = "thin"  # thin LTO lebih cepat compile, size hampir sama
codegen-units = 1
strip = true
panic = "abort"  # smaller binary

[profile.dev]
opt-level = 1  # faster compilation di dev

# Cross-compile ke Windows dari Linux
[target.x86_64-pc-windows-gnu]
linker = "x86_64-w64-mingw32-gcc"
```

***

### ðŸ—ï¸ 3. STRUKTUR KODE LENGKAP

**`src/main.rs`**

```rust
use anyhow::Result;
use tracing::{info, error};

mod config;
mod database;
mod document;
mod embedding;
mod utils;
mod worker;

use config::Settings;
use database::DbPool;
use worker::Worker;

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    utils::logger::init_logger()?;
    
    info!("ðŸš€ Starting RAG Embedding Worker...");
    
    // Load configuration
    let settings = Settings::load()?;
    info!("âœ… Configuration loaded");
    
    // Initialize database pool
    let db_pool = DbPool::new(&settings.database).await?;
    info!("âœ… Database connection established");
    
    // Create worker instance
    let worker = Worker::new(settings, db_pool).await?;
    info!("âœ… Worker initialized");
    
    // Run worker (akan block sampai error atau shutdown signal)
    match worker.run().await {
        Ok(_) => info!("Worker stopped gracefully"),
        Err(e) => error!("Worker error: {}", e),
    }
    
    Ok(())
}
```

***

**`src/lib.rs`**

```rust
pub mod config;
pub mod database;
pub mod document;
pub mod embedding;
pub mod utils;
pub mod worker;

pub use config::Settings;
pub use utils::error::WorkerError;
```

***

**`src/config/mod.rs`**

```rust
pub mod settings;
pub use settings::Settings;
```

**`src/config/settings.rs`**

```rust
use anyhow::Result;
use config::{Config, ConfigError, Environment, File};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct Settings {
    pub database: DatabaseConfig,
    pub embedding: EmbeddingConfig,
    pub chunking: ChunkingConfig,
    pub worker: WorkerConfig,
    pub llama_server: LlamaServerConfig,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct DatabaseConfig {
    pub url: String,
    pub pool_max_size: u32,
    pub pool_timeout_seconds: u64,
    pub listen_channel: String,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct EmbeddingConfig {
    pub model: String,  // model name untuk llama-server
    pub dimension: usize,  // 384 untuk AllMiniLML6V2, 1536 untuk OpenAI
    pub batch_size: usize,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ChunkingConfig {
    pub size: usize,
    pub overlap: usize,
    #[serde(default = "default_strategy")]
    pub strategy: ChunkStrategy,
}

fn default_strategy() -> ChunkStrategy {
    ChunkStrategy::Semantic
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum ChunkStrategy {
    Semantic,  // Semantic splitting (best untuk RAG)
    Fixed,     // Fixed size chunks
    Recursive, // Recursive character splitting
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct WorkerConfig {
    pub threads: usize,
    pub bulk_batch_size: usize,
    pub processing_timeout_seconds: u64,
    pub document_root_path: PathBuf,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct LlamaServerConfig {
    pub binary_path: PathBuf,  // path ke llama-server binary
    pub model_path: PathBuf,    // path ke model embedding
    pub host: String,
    pub port: u16,
    pub startup_timeout_seconds: u64,
    pub shutdown_timeout_seconds: u64,
    #[serde(default = "default_embedding_flag")]
    pub embedding_only: bool,
    #[serde(default = "default_ctx_size")]
    pub ctx_size: u32,
    #[serde(default = "default_threads")]
    pub threads: i32,
}

fn default_embedding_flag() -> bool {
    true  // embedding-only mode by default
}

fn default_ctx_size() -> u32 {
    2048
}

fn default_threads() -> i32 {
    4
}

impl Settings {
    pub fn load() -> Result<Self> {
        // Load from environment first
        dotenvy::dotenv().ok();
        
        let config = Config::builder()
            // Load from config file
            .add_source(File::with_name("config/settings").required(false))
            // Override with environment variables (prefix: APP)
            // Example: APP_DATABASE__URL=postgres://...
            .add_source(
                Environment::with_prefix("APP")
                    .separator("__")
                    .try_parsing(true)
            )
            .build()?;
        
        let settings: Settings = config.try_deserialize()?;
        
        // Validate settings
        settings.validate()?;
        
        Ok(settings)
    }
    
    fn validate(&self) -> Result<()> {
        // Validate llama-server binary exists
        if !self.llama_server.binary_path.exists() {
            anyhow::bail!(
                "llama-server binary not found at: {:?}",
                self.llama_server.binary_path
            );
        }
        
        // Validate model exists
        if !self.llama_server.model_path.exists() {
            anyhow::bail!(
                "Embedding model not found at: {:?}",
                self.llama_server.model_path
            );
        }
        
        // Validate document root path
        if !self.worker.document_root_path.exists() {
            anyhow::bail!(
                "Document root path not found: {:?}",
                self.worker.document_root_path
            );
        }
        
        Ok(())
    }
}
```

***

**`src/database/mod.rs`**

```rust
pub mod models;
pub mod pool;
pub mod repository;
pub mod listener;

pub use models::*;
pub use pool::DbPool;
pub use repository::Repository;
pub use listener::NotificationListener;
```

**`src/database/pool.rs`**

```rust
use crate::config::DatabaseConfig;
use anyhow::Result;
use sqlx::{postgres::PgPoolOptions, PgPool};
use std::time::Duration;

#[derive(Clone)]
pub struct DbPool {
    pool: PgPool,
}

impl DbPool {
    pub async fn new(config: &DatabaseConfig) -> Result<Self> {
        let pool = PgPoolOptions::new()
            .max_connections(config.pool_max_size)
            .acquire_timeout(Duration::from_secs(config.pool_timeout_seconds))
            .connect(&config.url)
            .await?;
        
        // Test connection
        sqlx::query("SELECT 1").execute(&pool).await?;
        
        Ok(Self { pool })
    }
    
    pub fn get_pool(&self) -> &PgPool {
        &self.pool
    }
    
    pub async fn close(&self) {
        self.pool.close().await;
    }
}
```

**`src/database/models.rs`**

```rust
use chrono::{DateTime, Utc};
use pgvector::Vector;
use serde::{Deserialize, Serialize};
use sqlx::FromRow;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DocumentNotification {
    pub operation: String,  // INSERT, UPDATE
    pub document_id: i32,
    pub file_path: String,
    pub timestamp: f64,
}

#[derive(Debug, Clone, FromRow)]
pub struct DocumentFile {
    #[sqlx(rename = "DocumentID")]
    pub document_id: i32,
    #[sqlx(rename = "DocumentFilePath")]
    pub document_file_path: String,
}

#[derive(Debug, Clone)]
pub struct DocumentChunk {
    pub document_id: i32,
    pub tenant_id: Option<i32>,
    pub chunk_index: i32,
    pub content: String,
    pub char_count: i32,
    pub token_count: Option<i32>,
    pub embedding: Vector,
    pub page_number: Option<i32>,
    pub section: Option<String>,
    pub tags: Option<Vec<String>>,
}

#[derive(Debug, Clone, FromRow)]
pub struct IngestionLog {
    pub document_id: i32,
    pub file_path: String,
    pub file_size: Option<i64>,
    pub file_type: Option<String>,
    pub embedding_model: String,
    pub chunk_size: i32,
    pub chunk_overlap: i32,
    pub status: String,
    pub total_chunks: i32,
    pub processed_chunks: i32,
    pub last_error: Option<String>,
    pub retry_count: i32,
    pub started_at: Option<DateTime<Utc>>,
    pub processed_at: Option<DateTime<Utc>>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum IngestionStatus {
    Pending,
    Processing,
    Completed,
    Failed,
}

impl ToString for IngestionStatus {
    fn to_string(&self) -> String {
        match self {
            Self::Pending => "pending".to_string(),
            Self::Processing => "processing".to_string(),
            Self::Completed => "completed".to_string(),
            Self::Failed => "failed".to_string(),
        }
    }
}

impl From<String> for IngestionStatus {
    fn from(s: String) -> Self {
        match s.as_str() {
            "pending" => Self::Pending,
            "processing" => Self::Processing,
            "completed" => Self::Completed,
            "failed" => Self::Failed,
            _ => Self::Pending,
        }
    }
}
```

**`src/database/repository.rs`**

```rust
use super::{DbPool, DocumentChunk, DocumentFile, IngestionLog, IngestionStatus};
use anyhow::Result;
use pgvector::Vector;
use sqlx::Row;
use tracing::debug;

pub struct Repository {
    pool: DbPool,
}

impl Repository {
    pub fn new(pool: DbPool) -> Self {
        Self { pool }
    }
    
    // ==================== Document Files ====================
    
    pub async fn get_document_file(&self, document_id: i32) -> Result<Option<DocumentFile>> {
        let result = sqlx::query_as::<_, DocumentFile>(
            r#"SELECT "DocumentID", "DocumentFilePath" 
               FROM "TblDocumentFiles" 
               WHERE "DocumentID" = $1"#
        )
        .bind(document_id)
        .fetch_optional(self.pool.get_pool())
        .await?;
        
        Ok(result)
    }
    
    pub async fn get_all_document_files(&self) -> Result<Vec<DocumentFile>> {
        let results = sqlx::query_as::<_, DocumentFile>(
            r#"SELECT "DocumentID", "DocumentFilePath" 
               FROM "TblDocumentFiles"
               ORDER BY "DocumentID""#
        )
        .fetch_all(self.pool.get_pool())
        .await?;
        
        Ok(results)
    }
    
    // ==================== Chunks ====================
    
    pub async fn insert_chunks(&self, chunks: Vec<DocumentChunk>) -> Result<()> {
        if chunks.is_empty() {
            return Ok(());
        }
        
        let mut transaction = self.pool.get_pool().begin().await?;
        
        for chunk in chunks {
            sqlx::query(
                r#"INSERT INTO rag_document_chunks 
                   (document_id, tenant_id, chunk_index, content, char_count, 
                    token_count, embedding, page_number, section, tags)
                   VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                   ON CONFLICT (document_id, chunk_index) 
                   DO UPDATE SET 
                       content = EXCLUDED.content,
                       char_count = EXCLUDED.char_count,
                       token_count = EXCLUDED.token_count,
                       embedding = EXCLUDED.embedding,
                       page_number = EXCLUDED.page_number,
                       section = EXCLUDED.section,
                       tags = EXCLUDED.tags,
                       updated_at = now()"#
            )
            .bind(chunk.document_id)
            .bind(chunk.tenant_id)
            .bind(chunk.chunk_index)
            .bind(&chunk.content)
            .bind(chunk.char_count)
            .bind(chunk.token_count)
            .bind(chunk.embedding)
            .bind(chunk.page_number)
            .bind(chunk.section)
            .bind(chunk.tags)
            .execute(&mut *transaction)
            .await?;
        }
        
        transaction.commit().await?;
        debug!("Inserted {} chunks", chunks.len());
        
        Ok(())
    }
    
    pub async fn delete_chunks_by_document(&self, document_id: i32) -> Result<u64> {
        let result = sqlx::query(
            "DELETE FROM rag_document_chunks WHERE document_id = $1"
        )
        .bind(document_id)
        .execute(self.pool.get_pool())
        .await?;
        
        Ok(result.rows_affected())
    }
    
    pub async fn count_chunks_by_document(&self, document_id: i32) -> Result<i64> {
        let row = sqlx::query(
            "SELECT COUNT(*) as count FROM rag_document_chunks WHERE document_id = $1"
        )
        .bind(document_id)
        .fetch_one(self.pool.get_pool())
        .await?;
        
        Ok(row.get("count"))
    }
    
    // ==================== Ingestion Log ====================
    
    pub async fn upsert_ingestion_log(&self, log: &IngestionLog) -> Result<()> {
        sqlx::query(
            r#"INSERT INTO rag_ingestion_log 
               (document_id, file_path, file_size, file_type, 
                embedding_model, chunk_size, chunk_overlap, status,
                total_chunks, processed_chunks, last_error, retry_count,
                started_at, processed_at)
               VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
               ON CONFLICT (document_id) 
               DO UPDATE SET 
                   file_path = EXCLUDED.file_path,
                   file_size = EXCLUDED.file_size,
                   file_type = EXCLUDED.file_type,
                   embedding_model = EXCLUDED.embedding_model,
                   chunk_size = EXCLUDED.chunk_size,
                   chunk_overlap = EXCLUDED.chunk_overlap,
                   status = EXCLUDED.status,
                   total_chunks = EXCLUDED.total_chunks,
                   processed_chunks = EXCLUDED.processed_chunks,
                   last_error = EXCLUDED.last_error,
                   retry_count = EXCLUDED.retry_count,
                   started_at = COALESCE(EXCLUDED.started_at, rag_ingestion_log.started_at),
                   processed_at = EXCLUDED.processed_at,
                   updated_at = now()"#
        )
        .bind(log.document_id)
        .bind(&log.file_path)
        .bind(log.file_size)
        .bind(&log.file_type)
        .bind(&log.embedding_model)
        .bind(log.chunk_size)
        .bind(log.chunk_overlap)
        .bind(&log.status)
        .bind(log.total_chunks)
        .bind(log.processed_chunks)
        .bind(&log.last_error)
        .bind(log.retry_count)
        .bind(log.started_at)
        .bind(log.processed_at)
        .execute(self.pool.get_pool())
        .await?;
        
        Ok(())
    }
    
    pub async fn get_ingestion_log(&self, document_id: i32) -> Result<Option<IngestionLog>> {
        let result = sqlx::query_as::<_, IngestionLog>(
            "SELECT * FROM rag_ingestion_log WHERE document_id = $1"
        )
        .bind(document_id)
        .fetch_optional(self.pool.get_pool())
        .await?;
        
        Ok(result)
    }
    
    pub async fn update_ingestion_status(
        &self,
        document_id: i32,
        status: IngestionStatus,
        error: Option<String>
    ) -> Result<()> {
        sqlx::query(
            r#"UPDATE rag_ingestion_log 
               SET status = $2, 
                   last_error = $3,
                   processed_at = CASE WHEN $2 IN ('completed', 'failed') THEN now() ELSE processed_at END,
                   updated_at = now()
               WHERE document_id = $1"#
        )
        .bind(document_id)
        .bind(status.to_string())
        .bind(error)
        .execute(self.pool.get_pool())
        .await?;
        
        Ok(())
    }
}
```

Bro, ini baru bagian pertama (database layer). Masih ada:

- **`listener.rs`** - PostgreSQL LISTEN handler
- **`embedding/`** - Llama-server manager & HTTP client  
- **`document/`** - Multi-format parser & chunker
- **`worker/`** - Main processing logic & bulk indexer
- **`utils/`** - Error handling, logging, metrics
